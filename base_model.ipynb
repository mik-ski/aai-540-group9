{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a185b88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket: mlops-backblaze-d7b30cb5-us-east-1\n",
      "Region: us-east-1\n",
      "\n",
      "SageMaker Role: arn:aws:iam::656208180522:role/LabRole\n",
      "Feature Store Session initialized\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "from sagemaker.session import Session\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "ENV_PATH = \".env\"\n",
    "load_dotenv(ENV_PATH)\n",
    "\n",
    "# Get S3 bucket and region\n",
    "bucket = os.getenv(\"BUCKET_NAME\")\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name\n",
    "\n",
    "print(f\"Bucket: {bucket}\")\n",
    "print(f\"Region: {region}\")\n",
    "\n",
    "# Initialize SageMaker Feature Store session\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = os.getenv(\"SAGEMAKER_ROLE\")\n",
    "feature_store_session = Session(\n",
    "    boto_session=session,\n",
    "    sagemaker_client=boto3.client('sagemaker', region_name=region),\n",
    "    sagemaker_featurestore_runtime_client=boto3.client('sagemaker-featurestore-runtime', region_name=region)\n",
    ")\n",
    "\n",
    "print(f\"\\nSageMaker Role: {role}\")\n",
    "print(f\"Feature Store Session initialized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "445809ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FEATURE GROUP NAMES FROM .ENV\n",
      "================================================================================\n",
      "Training:     backblaze-hdd-failure-20260131-155325-train\n",
      "Validation:   backblaze-hdd-failure-20260131-155325-validation\n",
      "Test:         backblaze-hdd-failure-20260131-155325-test\n",
      "Production:   backblaze-hdd-failure-20260131-155325-production\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Load Feature Group names from .env\n",
    "feature_group_train = os.getenv(\"FEATURE_GROUP_TRAIN\")\n",
    "feature_group_val = os.getenv(\"FEATURE_GROUP_VAL\")\n",
    "feature_group_test = os.getenv(\"FEATURE_GROUP_TEST\")\n",
    "feature_group_prod = os.getenv(\"FEATURE_GROUP_PROD\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"FEATURE GROUP NAMES FROM .ENV\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Training:     {feature_group_train}\")\n",
    "print(f\"Validation:   {feature_group_val}\")\n",
    "print(f\"Test:         {feature_group_test}\")\n",
    "print(f\"Production:   {feature_group_prod}\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c90241af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_feature_group_to_dataframe(feature_group_name, feature_store_session, use_offline_store=False):\n",
    "    \"\"\"\n",
    "    Load a Feature Group from SageMaker Feature Store into a pandas DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    feature_group_name : str\n",
    "        Name of the feature group to load\n",
    "    feature_store_session : sagemaker.Session\n",
    "        SageMaker session for Feature Store\n",
    "    use_offline_store : bool\n",
    "        If True, use Athena to query offline store. If False, read from S3 parquet directly.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame : DataFrame containing all records from the feature group\n",
    "    \"\"\"\n",
    "    import time\n",
    "    from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "    \n",
    "    print(f\"\\nLoading Feature Group: {feature_group_name}\")\n",
    "    \n",
    "    # Create Feature Group object\n",
    "    feature_group = FeatureGroup(\n",
    "        name=feature_group_name,\n",
    "        sagemaker_session=feature_store_session\n",
    "    )\n",
    "    \n",
    "    # Check status\n",
    "    try:\n",
    "        description = feature_group.describe()\n",
    "        status = description.get('FeatureGroupStatus')\n",
    "        print(f\"  Status: {status}\")\n",
    "        \n",
    "        if status != 'Created':\n",
    "            print(f\"  ⚠ Warning: Feature Group is not in 'Created' state\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Error: Feature Group not found - {str(e)[:100]}\")\n",
    "        return None\n",
    "    \n",
    "    if use_offline_store:\n",
    "        # Use Athena query to retrieve data from offline store\n",
    "        query_string = f'SELECT * FROM \"{feature_group_name}\"'\n",
    "        \n",
    "        print(f\"  Querying offline store via Athena...\")\n",
    "        print(f\"  Query: {query_string}\")\n",
    "        \n",
    "        try:\n",
    "            # Create Athena query\n",
    "            athena_query = feature_group.athena_query()\n",
    "            \n",
    "            # Set database and table\n",
    "            athena_query.run(\n",
    "                query_string=query_string,\n",
    "                output_location=f\"s3://{bucket}/athena-results/\"\n",
    "            )\n",
    "            \n",
    "            # Wait for query to complete\n",
    "            athena_query.wait()\n",
    "            \n",
    "            # Get results as DataFrame\n",
    "            df = athena_query.as_dataframe()\n",
    "            \n",
    "            print(f\"  ✓ Loaded {len(df):,} records\")\n",
    "            print(f\"  Columns: {list(df.columns)}\")\n",
    "            \n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error loading data via Athena: {str(e)[:200]}\")\n",
    "            print(f\"\\n  Note: Offline store data may take a few minutes to be available.\")\n",
    "            print(f\"  Trying direct S3 read instead...\")\n",
    "    \n",
    "    # Read directly from S3 parquet files (faster and works immediately)\n",
    "    try:\n",
    "        print(f\"  Reading parquet files from S3...\")\n",
    "        \n",
    "        # Get S3 URI for offline store - use the resolved URI from description\n",
    "        offline_store_config = description.get('OfflineStoreConfig', {})\n",
    "        resolved_output_s3_uri = offline_store_config.get('S3StorageConfig', {}).get('ResolvedOutputS3Uri', '')\n",
    "        \n",
    "        if not resolved_output_s3_uri:\n",
    "            # Fallback: construct from bucket  \n",
    "            resolved_output_s3_uri = f\"s3://{bucket}/feature-store/{feature_group_name}\"\n",
    "        \n",
    "        # The resolved URI already points to the base path; parquet files are in /data subdirectory\n",
    "        # Check if it already ends with /data, if not append it\n",
    "        if resolved_output_s3_uri.endswith('/data'):\n",
    "            data_s3_uri = resolved_output_s3_uri\n",
    "        else:\n",
    "            data_s3_uri = f\"{resolved_output_s3_uri}/data\"\n",
    "        \n",
    "        print(f\"  S3 Path: {data_s3_uri}\")\n",
    "        \n",
    "        # Read parquet files using pandas\n",
    "        import pyarrow.parquet as pq\n",
    "        import pyarrow.dataset as ds\n",
    "        \n",
    "        # Read the dataset (partitioned by year/month/day/hour)\n",
    "        dataset = ds.dataset(data_s3_uri, format='parquet', partitioning='hive')\n",
    "        df = dataset.to_table().to_pandas()\n",
    "        \n",
    "        # Drop internal columns and partition columns\n",
    "        columns_to_drop = [col for col in df.columns if col in ['write_time', 'api_invocation_time', 'is_deleted', 'year', 'month', 'day', 'hour']]\n",
    "        if columns_to_drop:\n",
    "            df = df.drop(columns=columns_to_drop)\n",
    "        \n",
    "        print(f\"  ✓ Loaded {len(df):,} records\")\n",
    "        print(f\"  Columns: {list(df.columns)[:10]}...\" if len(df.columns) > 10 else f\"  Columns: {list(df.columns)}\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Error loading data from S3: {str(e)[:200]}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "32b9010b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking S3 structure for feature store...\n",
      "\n",
      "Top-level directories under feature-store/:\n",
      "  feature-store/backblaze-hdd-failure-20260131-145420-production/\n",
      "  feature-store/backblaze-hdd-failure-20260131-145420-test/\n",
      "  feature-store/backblaze-hdd-failure-20260131-145420-train/\n",
      "  feature-store/backblaze-hdd-failure-20260131-145420-validation/\n",
      "  feature-store/backblaze-hdd-failure-20260131-150433-production/\n",
      "  feature-store/backblaze-hdd-failure-20260131-150433-test/\n",
      "  feature-store/backblaze-hdd-failure-20260131-150433-train/\n",
      "  feature-store/backblaze-hdd-failure-20260131-150433-validation/\n",
      "  feature-store/backblaze-hdd-failure-20260131-155325-production/\n",
      "  feature-store/backblaze-hdd-failure-20260131-155325-test/\n",
      "\n",
      "Checking structure under feature-store/backblaze-hdd-failure-20260131-155325-train/:\n",
      "  feature-store/backblaze-hdd-failure-20260131-155325-train/656208180522/sagemaker/us-east-1/offline-store/backblaze-hdd-failure-20260131-155325-train-1769874829/backblaze-hdd-failure-20260131-155325-train2026-01-31T15:53:49.568Z.txt\n",
      "  feature-store/backblaze-hdd-failure-20260131-155325-train/656208180522/sagemaker/us-east-1/offline-store/backblaze-hdd-failure-20260131-155325-train-1769874829/data/year=2026/month=01/day=31/hour=15/20260131T155324Z_0871DYcbmNhz1GSu.parquet\n",
      "  feature-store/backblaze-hdd-failure-20260131-155325-train/656208180522/sagemaker/us-east-1/offline-store/backblaze-hdd-failure-20260131-155325-train-1769874829/data/year=2026/month=01/day=31/hour=15/20260131T155324Z_0QQZl2gwob87IlXD.parquet\n",
      "  feature-store/backblaze-hdd-failure-20260131-155325-train/656208180522/sagemaker/us-east-1/offline-store/backblaze-hdd-failure-20260131-155325-train-1769874829/data/year=2026/month=01/day=31/hour=15/20260131T155324Z_0aJpNZPGXIX628lO.parquet\n",
      "  feature-store/backblaze-hdd-failure-20260131-155325-train/656208180522/sagemaker/us-east-1/offline-store/backblaze-hdd-failure-20260131-155325-train-1769874829/data/year=2026/month=01/day=31/hour=15/20260131T155324Z_0b36oe3dIgIBQ4pU.parquet\n",
      "  feature-store/backblaze-hdd-failure-20260131-155325-train/656208180522/sagemaker/us-east-1/offline-store/backblaze-hdd-failure-20260131-155325-train-1769874829/data/year=2026/month=01/day=31/hour=15/20260131T155324Z_0dDtwfohx46tmJTo.parquet\n",
      "  feature-store/backblaze-hdd-failure-20260131-155325-train/656208180522/sagemaker/us-east-1/offline-store/backblaze-hdd-failure-20260131-155325-train-1769874829/data/year=2026/month=01/day=31/hour=15/20260131T155324Z_0gOdxm64k40s8M6u.parquet\n",
      "  feature-store/backblaze-hdd-failure-20260131-155325-train/656208180522/sagemaker/us-east-1/offline-store/backblaze-hdd-failure-20260131-155325-train-1769874829/data/year=2026/month=01/day=31/hour=15/20260131T155324Z_0gaUBjPkf6nWjXe0.parquet\n",
      "  feature-store/backblaze-hdd-failure-20260131-155325-train/656208180522/sagemaker/us-east-1/offline-store/backblaze-hdd-failure-20260131-155325-train-1769874829/data/year=2026/month=01/day=31/hour=15/20260131T155324Z_0n2UJmlQfMER0u35.parquet\n",
      "  feature-store/backblaze-hdd-failure-20260131-155325-train/656208180522/sagemaker/us-east-1/offline-store/backblaze-hdd-failure-20260131-155325-train-1769874829/data/year=2026/month=01/day=31/hour=15/20260131T155324Z_0qNQO2ttUqp0IkhQ.parquet\n"
     ]
    }
   ],
   "source": [
    "# Check the actual S3 structure for one feature group\n",
    "s3_client = boto3.client('s3', region_name=region)\n",
    "\n",
    "print(\"Checking S3 structure for feature store...\")\n",
    "prefix = \"feature-store/\"\n",
    "response = s3_client.list_objects_v2(Bucket=bucket, Prefix=prefix, Delimiter='/', MaxKeys=10)\n",
    "\n",
    "print(f\"\\nTop-level directories under {prefix}:\")\n",
    "for item in response.get('CommonPrefixes', []):\n",
    "    print(f\"  {item['Prefix']}\")\n",
    "\n",
    "# Check one feature group in detail\n",
    "if feature_group_train:\n",
    "    fg_prefix = f\"feature-store/{feature_group_train}/\"\n",
    "    print(f\"\\nChecking structure under {fg_prefix}:\")\n",
    "    response = s3_client.list_objects_v2(Bucket=bucket, Prefix=fg_prefix, MaxKeys=20)\n",
    "    \n",
    "    for obj in response.get('Contents', [])[:10]:\n",
    "        print(f\"  {obj['Key']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dfe4a2cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LOADING FEATURE GROUPS INTO DATAFRAMES\n",
      "================================================================================\n",
      "\n",
      "Loading Feature Group: backblaze-hdd-failure-20260131-155325-train\n",
      "  Status: Created\n",
      "  Reading parquet files from S3...\n",
      "  S3 Path: s3://mlops-backblaze-d7b30cb5-us-east-1/feature-store/backblaze-hdd-failure-20260131-155325-train/656208180522/sagemaker/us-east-1/offline-store/backblaze-hdd-failure-20260131-155325-train-1769874829/data\n",
      "  ✓ Loaded 651,891 records\n",
      "  Columns: ['record_id', 'event_time', 'serial_number', 'date', 'pct_one_star', 'pct_two_star', 'smart_5_raw', 'smart_187_raw', 'smart_188_raw', 'smart_197_raw']...\n",
      "\n",
      "Loading Feature Group: backblaze-hdd-failure-20260131-155325-validation\n",
      "  Status: Created\n",
      "  Reading parquet files from S3...\n",
      "  S3 Path: s3://mlops-backblaze-d7b30cb5-us-east-1/feature-store/backblaze-hdd-failure-20260131-155325-validation/656208180522/sagemaker/us-east-1/offline-store/backblaze-hdd-failure-20260131-155325-validation-1769874830/data\n",
      "  ✓ Loaded 162,918 records\n",
      "  Columns: ['record_id', 'event_time', 'serial_number', 'date', 'pct_one_star', 'pct_two_star', 'smart_5_raw', 'smart_187_raw', 'smart_188_raw', 'smart_197_raw']...\n",
      "\n",
      "Loading Feature Group: backblaze-hdd-failure-20260131-155325-test\n",
      "  Status: Created\n",
      "  Reading parquet files from S3...\n",
      "  S3 Path: s3://mlops-backblaze-d7b30cb5-us-east-1/feature-store/backblaze-hdd-failure-20260131-155325-test/656208180522/sagemaker/us-east-1/offline-store/backblaze-hdd-failure-20260131-155325-test-1769874832/data\n",
      "  ✓ Loaded 162,962 records\n",
      "  Columns: ['record_id', 'event_time', 'serial_number', 'date', 'pct_one_star', 'pct_two_star', 'smart_5_raw', 'smart_187_raw', 'smart_188_raw', 'smart_197_raw']...\n",
      "\n",
      "Loading Feature Group: backblaze-hdd-failure-20260131-155325-production\n",
      "  Status: Created\n",
      "  Reading parquet files from S3...\n",
      "  S3 Path: s3://mlops-backblaze-d7b30cb5-us-east-1/feature-store/backblaze-hdd-failure-20260131-155325-production/656208180522/sagemaker/us-east-1/offline-store/backblaze-hdd-failure-20260131-155325-production-1769874833/data\n",
      "  ✓ Loaded 651,878 records\n",
      "  Columns: ['record_id', 'event_time', 'serial_number', 'date', 'pct_one_star', 'pct_two_star', 'smart_5_raw', 'smart_187_raw', 'smart_188_raw', 'smart_197_raw']...\n",
      "\n",
      "================================================================================\n",
      "LOADING SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Training Dataset:\n",
      "  Shape: (651891, 12)\n",
      "  Failures: 50\n",
      "\n",
      "Validation Dataset:\n",
      "  Shape: (162918, 12)\n",
      "  Failures: 19\n",
      "\n",
      "Test Dataset:\n",
      "  Shape: (162962, 12)\n",
      "  Failures: 8\n",
      "\n",
      "Production Dataset:\n",
      "  Shape: (651878, 12)\n",
      "  Failures: 53\n"
     ]
    }
   ],
   "source": [
    "# Load all feature groups into dataframes\n",
    "print(\"=\"*80)\n",
    "print(\"LOADING FEATURE GROUPS INTO DATAFRAMES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load training data\n",
    "df_train = load_feature_group_to_dataframe(feature_group_train, feature_store_session)\n",
    "\n",
    "# Load validation data\n",
    "df_val = load_feature_group_to_dataframe(feature_group_val, feature_store_session)\n",
    "\n",
    "# Load test data\n",
    "df_test = load_feature_group_to_dataframe(feature_group_test, feature_store_session)\n",
    "\n",
    "# Load production data\n",
    "df_prod = load_feature_group_to_dataframe(feature_group_prod, feature_store_session)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LOADING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "datasets = {\n",
    "    'Training': df_train,\n",
    "    'Validation': df_val,\n",
    "    'Test': df_test,\n",
    "    'Production': df_prod\n",
    "}\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    if df is not None:\n",
    "        print(f\"\\n{name} Dataset:\")\n",
    "        print(f\"  Shape: {df.shape}\")\n",
    "        print(f\"  Failures: {df['failure'].sum() if 'failure' in df.columns else 'N/A'}\")\n",
    "    else:\n",
    "        print(f\"\\n{name} Dataset: Failed to load\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "71aa3b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SAMPLE DATA FROM TRAINING SET\n",
      "================================================================================\n",
      "\n",
      "First 5 rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>record_id</th>\n",
       "      <th>event_time</th>\n",
       "      <th>serial_number</th>\n",
       "      <th>date</th>\n",
       "      <th>pct_one_star</th>\n",
       "      <th>pct_two_star</th>\n",
       "      <th>smart_5_raw</th>\n",
       "      <th>smart_187_raw</th>\n",
       "      <th>smart_188_raw</th>\n",
       "      <th>smart_197_raw</th>\n",
       "      <th>smart_198_raw</th>\n",
       "      <th>failure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ZL23V7T1_2025-09-24</td>\n",
       "      <td>1.769875e+09</td>\n",
       "      <td>ZL23V7T1</td>\n",
       "      <td>2025-09-24</td>\n",
       "      <td>25.12</td>\n",
       "      <td>7.27</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22E0A21AFV8G_2025-09-25</td>\n",
       "      <td>1.769875e+09</td>\n",
       "      <td>22E0A21AFV8G</td>\n",
       "      <td>2025-09-25</td>\n",
       "      <td>17.01</td>\n",
       "      <td>6.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22G0A08AFV8G_2025-09-26</td>\n",
       "      <td>1.769875e+09</td>\n",
       "      <td>22G0A08AFV8G</td>\n",
       "      <td>2025-09-26</td>\n",
       "      <td>17.01</td>\n",
       "      <td>6.10</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8160A08HFV8G_2025-09-25</td>\n",
       "      <td>1.769875e+09</td>\n",
       "      <td>8160A08HFV8G</td>\n",
       "      <td>2025-09-25</td>\n",
       "      <td>17.01</td>\n",
       "      <td>6.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8160A096FV8G_2025-09-24</td>\n",
       "      <td>1.769875e+09</td>\n",
       "      <td>8160A096FV8G</td>\n",
       "      <td>2025-09-24</td>\n",
       "      <td>17.01</td>\n",
       "      <td>6.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 record_id    event_time serial_number        date  \\\n",
       "0      ZL23V7T1_2025-09-24  1.769875e+09      ZL23V7T1  2025-09-24   \n",
       "1  22E0A21AFV8G_2025-09-25  1.769875e+09  22E0A21AFV8G  2025-09-25   \n",
       "2  22G0A08AFV8G_2025-09-26  1.769875e+09  22G0A08AFV8G  2025-09-26   \n",
       "3  8160A08HFV8G_2025-09-25  1.769875e+09  8160A08HFV8G  2025-09-25   \n",
       "4  8160A096FV8G_2025-09-24  1.769875e+09  8160A096FV8G  2025-09-24   \n",
       "\n",
       "   pct_one_star  pct_two_star  smart_5_raw  smart_187_raw  smart_188_raw  \\\n",
       "0         25.12          7.27          0.0            0.0            0.0   \n",
       "1         17.01          6.10          0.0            0.0            0.0   \n",
       "2         17.01          6.10          3.0            0.0            0.0   \n",
       "3         17.01          6.10          0.0            0.0            0.0   \n",
       "4         17.01          6.10          0.0            0.0            0.0   \n",
       "\n",
       "   smart_197_raw  smart_198_raw  failure  \n",
       "0            0.0            0.0        0  \n",
       "1            0.0            0.0        0  \n",
       "2            0.0            0.0        0  \n",
       "3            0.0            0.0        0  \n",
       "4            0.0            0.0        0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data types:\n",
      "record_id         object\n",
      "event_time       float64\n",
      "serial_number     object\n",
      "date              object\n",
      "pct_one_star     float64\n",
      "pct_two_star     float64\n",
      "smart_5_raw      float64\n",
      "smart_187_raw    float64\n",
      "smart_188_raw    float64\n",
      "smart_197_raw    float64\n",
      "smart_198_raw    float64\n",
      "failure            int64\n",
      "dtype: object\n",
      "\n",
      "Basic statistics:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_time</th>\n",
       "      <th>pct_one_star</th>\n",
       "      <th>pct_two_star</th>\n",
       "      <th>smart_5_raw</th>\n",
       "      <th>smart_187_raw</th>\n",
       "      <th>smart_188_raw</th>\n",
       "      <th>smart_197_raw</th>\n",
       "      <th>smart_198_raw</th>\n",
       "      <th>failure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6.518910e+05</td>\n",
       "      <td>651891.000000</td>\n",
       "      <td>651891.000000</td>\n",
       "      <td>651891.000000</td>\n",
       "      <td>651891.000000</td>\n",
       "      <td>6.518910e+05</td>\n",
       "      <td>651891.000000</td>\n",
       "      <td>651891.000000</td>\n",
       "      <td>651891.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.769875e+09</td>\n",
       "      <td>18.839298</td>\n",
       "      <td>6.368991</td>\n",
       "      <td>45.554063</td>\n",
       "      <td>2.649619</td>\n",
       "      <td>3.957041e+08</td>\n",
       "      <td>1.813253</td>\n",
       "      <td>1.175213</td>\n",
       "      <td>0.000077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.711874</td>\n",
       "      <td>0.675577</td>\n",
       "      <td>1152.373257</td>\n",
       "      <td>330.320273</td>\n",
       "      <td>1.386535e+10</td>\n",
       "      <td>66.307377</td>\n",
       "      <td>50.599231</td>\n",
       "      <td>0.008758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.769875e+09</td>\n",
       "      <td>13.550000</td>\n",
       "      <td>5.460000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.769875e+09</td>\n",
       "      <td>15.450000</td>\n",
       "      <td>5.770000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.769875e+09</td>\n",
       "      <td>17.010000</td>\n",
       "      <td>6.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.769875e+09</td>\n",
       "      <td>25.120000</td>\n",
       "      <td>7.270000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.769875e+09</td>\n",
       "      <td>25.120000</td>\n",
       "      <td>7.270000</td>\n",
       "      <td>65528.000000</td>\n",
       "      <td>65535.000000</td>\n",
       "      <td>2.207647e+12</td>\n",
       "      <td>17573.000000</td>\n",
       "      <td>17573.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         event_time   pct_one_star   pct_two_star    smart_5_raw  \\\n",
       "count  6.518910e+05  651891.000000  651891.000000  651891.000000   \n",
       "mean   1.769875e+09      18.839298       6.368991      45.554063   \n",
       "std    0.000000e+00       4.711874       0.675577    1152.373257   \n",
       "min    1.769875e+09      13.550000       5.460000       0.000000   \n",
       "25%    1.769875e+09      15.450000       5.770000       0.000000   \n",
       "50%    1.769875e+09      17.010000       6.100000       0.000000   \n",
       "75%    1.769875e+09      25.120000       7.270000       0.000000   \n",
       "max    1.769875e+09      25.120000       7.270000   65528.000000   \n",
       "\n",
       "       smart_187_raw  smart_188_raw  smart_197_raw  smart_198_raw  \\\n",
       "count  651891.000000   6.518910e+05  651891.000000  651891.000000   \n",
       "mean        2.649619   3.957041e+08       1.813253       1.175213   \n",
       "std       330.320273   1.386535e+10      66.307377      50.599231   \n",
       "min         0.000000   0.000000e+00       0.000000       0.000000   \n",
       "25%         0.000000   0.000000e+00       0.000000       0.000000   \n",
       "50%         0.000000   0.000000e+00       0.000000       0.000000   \n",
       "75%         0.000000   0.000000e+00       0.000000       0.000000   \n",
       "max     65535.000000   2.207647e+12   17573.000000   17573.000000   \n",
       "\n",
       "             failure  \n",
       "count  651891.000000  \n",
       "mean        0.000077  \n",
       "std         0.008758  \n",
       "min         0.000000  \n",
       "25%         0.000000  \n",
       "50%         0.000000  \n",
       "75%         0.000000  \n",
       "max         1.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display sample data from training set\n",
    "if df_train is not None:\n",
    "    print(\"=\"*80)\n",
    "    print(\"SAMPLE DATA FROM TRAINING SET\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nFirst 5 rows:\")\n",
    "    display(df_train.head())\n",
    "    \n",
    "    print(f\"\\nData types:\")\n",
    "    print(df_train.dtypes)\n",
    "    \n",
    "    print(f\"\\nBasic statistics:\")\n",
    "    display(df_train.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9151882d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PREPARING FEATURE MATRICES AND TARGET VECTORS\n",
      "================================================================================\n",
      "\n",
      "Training Set:\n",
      "  X_train: (651891, 7)\n",
      "  y_train: (651891,)\n",
      "  Failure rate: 0.0077%\n",
      "\n",
      "Validation Set:\n",
      "  X_val: (162918, 7)\n",
      "  y_val: (162918,)\n",
      "  Failure rate: 0.0117%\n",
      "\n",
      "Test Set:\n",
      "  X_test: (162962, 7)\n",
      "  y_test: (162962,)\n",
      "  Failure rate: 0.0049%\n",
      "\n",
      "Production Set:\n",
      "  X_prod: (651878, 7)\n",
      "  y_prod: (651878,)\n",
      "  Failure rate: 0.0081%\n",
      "\n",
      "================================================================================\n",
      "Feature columns (7):\n",
      "  1. pct_one_star\n",
      "  2. pct_two_star\n",
      "  3. smart_5_raw\n",
      "  4. smart_187_raw\n",
      "  5. smart_188_raw\n",
      "  6. smart_197_raw\n",
      "  7. smart_198_raw\n"
     ]
    }
   ],
   "source": [
    "# Prepare X and y datasets for modeling\n",
    "feature_cols = ['pct_one_star', 'pct_two_star', 'smart_5_raw', \n",
    "                'smart_187_raw', 'smart_188_raw', 'smart_197_raw', 'smart_198_raw']\n",
    "\n",
    "if df_train is not None and df_val is not None and df_test is not None:\n",
    "    print(\"=\"*80)\n",
    "    print(\"PREPARING FEATURE MATRICES AND TARGET VECTORS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Training data\n",
    "    X_train = df_train[feature_cols].copy()\n",
    "    y_train = df_train['failure'].copy()\n",
    "    \n",
    "    # Validation data\n",
    "    X_val = df_val[feature_cols].copy()\n",
    "    y_val = df_val['failure'].copy()\n",
    "    \n",
    "    # Test data\n",
    "    X_test = df_test[feature_cols].copy()\n",
    "    y_test = df_test['failure'].copy()\n",
    "    \n",
    "    # Production data (if needed)\n",
    "    if df_prod is not None:\n",
    "        X_prod = df_prod[feature_cols].copy()\n",
    "        y_prod = df_prod['failure'].copy()\n",
    "    \n",
    "    print(f\"\\nTraining Set:\")\n",
    "    print(f\"  X_train: {X_train.shape}\")\n",
    "    print(f\"  y_train: {y_train.shape}\")\n",
    "    print(f\"  Failure rate: {y_train.sum() / len(y_train) * 100:.4f}%\")\n",
    "    \n",
    "    print(f\"\\nValidation Set:\")\n",
    "    print(f\"  X_val: {X_val.shape}\")\n",
    "    print(f\"  y_val: {y_val.shape}\")\n",
    "    print(f\"  Failure rate: {y_val.sum() / len(y_val) * 100:.4f}%\")\n",
    "    \n",
    "    print(f\"\\nTest Set:\")\n",
    "    print(f\"  X_test: {X_test.shape}\")\n",
    "    print(f\"  y_test: {y_test.shape}\")\n",
    "    print(f\"  Failure rate: {y_test.sum() / len(y_test) * 100:.4f}%\")\n",
    "    \n",
    "    if df_prod is not None:\n",
    "        print(f\"\\nProduction Set:\")\n",
    "        print(f\"  X_prod: {X_prod.shape}\")\n",
    "        print(f\"  y_prod: {y_prod.shape}\")\n",
    "        print(f\"  Failure rate: {y_prod.sum() / len(y_prod) * 100:.4f}%\")\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(f\"Feature columns ({len(feature_cols)}):\")\n",
    "    for i, col in enumerate(feature_cols, 1):\n",
    "        print(f\"  {i}. {col}\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\n⚠ Warning: Not all datasets loaded successfully. Cannot prepare feature matrices.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
