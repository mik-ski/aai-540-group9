{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4614eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing bucket from .env: mlops-backblaze-d7b30cb5-us-east-1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import boto3\n",
    "from dotenv import load_dotenv, set_key\n",
    "\n",
    "ENV_PATH = \".env\"\n",
    "\n",
    "# Load .env if it exists\n",
    "load_dotenv(ENV_PATH)\n",
    "\n",
    "bucket = os.getenv(\"BUCKET_NAME\")\n",
    "\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name\n",
    "s3 = boto3.client(\"s3\", region_name=region)\n",
    "\n",
    "# If BUCKET_NAME exists, use it\n",
    "if bucket:\n",
    "    print(\"Using existing bucket from .env:\", bucket)\n",
    "\n",
    "else:\n",
    "    # Create new bucket name\n",
    "    print(\"Run load_data.ipynb to create a new S3 bucket and load data into it.\")\n",
    "    raise Exception(\"BUCKET_NAME not found in .env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850597c4",
   "metadata": {},
   "source": [
    "## Read Backblaze Parquet Files with AWS Glue\n",
    "\n",
    "Use AWS Glue Interactive Sessions to read the partitioned parquet files with a managed Spark environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b12112c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AWS Glue client initialized\n",
      "Region: us-east-1\n",
      "\n",
      "Glue can be used to:\n",
      "  1. Query the Athena table we created\n",
      "  2. Use Glue Data Catalog for schema management\n",
      "  3. Run Glue ETL jobs for transformations\n",
      "  4. Use Glue Interactive Sessions (requires Glue kernel)\n"
     ]
    }
   ],
   "source": [
    "# Install and configure AWS Glue Sessions\n",
    "# Note: For Glue Interactive Sessions, you need to use the Glue kernel or %%configure magic\n",
    "# Here we'll use the boto3 approach to work with Glue Data Catalog\n",
    "\n",
    "# Initialize Glue client\n",
    "glue_client = boto3.client('glue', region_name=region)\n",
    "\n",
    "print(\"AWS Glue client initialized\")\n",
    "print(f\"Region: {region}\")\n",
    "print(f\"\\nGlue can be used to:\")\n",
    "print(\"  1. Query the Athena table we created\")\n",
    "print(\"  2. Use Glue Data Catalog for schema management\")\n",
    "print(\"  3. Run Glue ETL jobs for transformations\")\n",
    "print(\"  4. Use Glue Interactive Sessions (requires Glue kernel)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5734a2e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Backblaze parquet dataset from S3 using PyArrow...\n",
      "S3 Path: s3://mlops-backblaze-d7b30cb5-us-east-1/curated/backblaze_parquet/\n",
      "\n",
      "Dataset loaded!\n",
      "\n",
      "Dataset schema (first 30 fields):\n",
      "  1. date: large_string\n",
      "  2. serial_number: large_string\n",
      "  3. model: large_string\n",
      "  4. capacity_bytes: int64\n",
      "  5. failure: int64\n",
      "  6. datacenter: large_string\n",
      "  7. cluster_id: int64\n",
      "  8. vault_id: int64\n",
      "  9. pod_id: int64\n",
      "  10. pod_slot_num: double\n",
      "  11. is_legacy_format: bool\n",
      "  12. smart_1_normalized: double\n",
      "  13. smart_1_raw: double\n",
      "  14. smart_2_normalized: double\n",
      "  15. smart_2_raw: double\n",
      "  16. smart_3_normalized: double\n",
      "  17. smart_3_raw: double\n",
      "  18. smart_4_normalized: double\n",
      "  19. smart_4_raw: double\n",
      "  20. smart_5_normalized: double\n",
      "  21. smart_5_raw: double\n",
      "  22. smart_7_normalized: double\n",
      "  23. smart_7_raw: double\n",
      "  24. smart_8_normalized: double\n",
      "  25. smart_8_raw: double\n",
      "  26. smart_9_normalized: double\n",
      "  27. smart_9_raw: double\n",
      "  28. smart_10_normalized: double\n",
      "  29. smart_10_raw: double\n",
      "  30. smart_11_normalized: double\n",
      "\n",
      "  ... and 170 more fields\n",
      "\n",
      "Total fields: 200\n",
      "Partitioning: <pyarrow._dataset.HivePartitioning object at 0x7ff2df59e7e0>\n",
      "Format: <ParquetFileFormat read_options=<ParquetReadOptions dictionary_columns=set() coerce_int96_timestamp_unit=ns>>\n"
     ]
    }
   ],
   "source": [
    "# Read Backblaze parquet files using PyArrow and S3\n",
    "# This is a lightweight alternative to Spark when Glue Interactive Sessions aren't available\n",
    "\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.dataset as ds\n",
    "import pyarrow.compute as pc\n",
    "\n",
    "print(\"Reading Backblaze parquet dataset from S3 using PyArrow...\")\n",
    "print(f\"S3 Path: s3://{bucket}/curated/backblaze_parquet/\\n\")\n",
    "\n",
    "# Create dataset from S3 path\n",
    "# PyArrow can efficiently read partitioned parquet files\n",
    "dataset = ds.dataset(\n",
    "    f\"s3://{bucket}/curated/backblaze_parquet/\",\n",
    "    format=\"parquet\",\n",
    "    partitioning=\"hive\"  # Recognizes year=/month=/day= partitioning\n",
    ")\n",
    "\n",
    "print(f\"Dataset loaded!\")\n",
    "print(f\"\\nDataset schema (first 30 fields):\")\n",
    "schema_fields = list(dataset.schema)\n",
    "for i, field in enumerate(schema_fields[:30], 1):\n",
    "    print(f\"  {i}. {field.name}: {field.type}\")\n",
    "\n",
    "if len(schema_fields) > 30:\n",
    "    print(f\"\\n  ... and {len(schema_fields) - 30} more fields\")\n",
    "\n",
    "print(f\"\\nTotal fields: {len(schema_fields)}\")\n",
    "print(f\"Partitioning: {dataset.partitioning}\")\n",
    "print(f\"Format: {dataset.format}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05400784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data into PyArrow Table...\n",
      "Reading the last 5 days of data...\n",
      "\n",
      "Total unique date partitions available: 273\n",
      "Last few partitions: ['(((year == 2025) and (month == 9)) and (day == 5))', '(((year == 2025) and (month == 9)) and (day == 6))', '(((year == 2025) and (month == 9)) and (day == 7))', '(((year == 2025) and (month == 9)) and (day == 8))', '(((year == 2025) and (month == 9)) and (day == 9))']\n",
      "\n",
      "Selected 5 days:\n",
      "  - 2025-09-23\n",
      "  - 2025-09-24\n",
      "  - 2025-09-25\n",
      "  - 2025-09-26\n",
      "  - 2025-09-27\n",
      "\n",
      "Data loaded from last 5 days!\n",
      "Rows: 1,645,930\n",
      "Columns: 200\n",
      "Size in memory: ~2581.4 MB\n",
      "\n",
      "First 5 rows (showing ALL 200 columns):\n",
      "         date serial_number           model  capacity_bytes  failure  \\\n",
      "0  2025-09-23  2206E608DB42  CT250MX500SSD1    250059350016        0   \n",
      "1  2025-09-23  2207E60CC65A  CT250MX500SSD1    250059350016        0   \n",
      "2  2025-09-23  2340E87B92B5  CT250MX500SSD1    250059350016        0   \n",
      "3  2025-09-23  2340E87B97E8  CT250MX500SSD1    250059350016        0   \n",
      "4  2025-09-23  2407E896B6D5  CT250MX500SSD1    250059350016        0   \n",
      "\n",
      "  datacenter  cluster_id  vault_id  pod_id  pod_slot_num  ...  smart_251_raw  \\\n",
      "0       sac0           0      1028       4           NaN  ...            NaN   \n",
      "1       sac0           0      1028      13           NaN  ...            NaN   \n",
      "2       sac0           0      1028      14           NaN  ...            NaN   \n",
      "3       sac0           0      1028       2           NaN  ...            NaN   \n",
      "4       sac0           0      1028       8           NaN  ...            NaN   \n",
      "\n",
      "   smart_252_normalized  smart_252_raw  smart_254_normalized  smart_254_raw  \\\n",
      "0                   NaN            NaN                   NaN            NaN   \n",
      "1                   NaN            NaN                   NaN            NaN   \n",
      "2                   NaN            NaN                   NaN            NaN   \n",
      "3                   NaN            NaN                   NaN            NaN   \n",
      "4                   NaN            NaN                   NaN            NaN   \n",
      "\n",
      "   smart_255_normalized  smart_255_raw  year  month  day  \n",
      "0                   NaN            NaN  2025      9   23  \n",
      "1                   NaN            NaN  2025      9   23  \n",
      "2                   NaN            NaN  2025      9   23  \n",
      "3                   NaN            NaN  2025      9   23  \n",
      "4                   NaN            NaN  2025      9   23  \n",
      "\n",
      "[5 rows x 200 columns]\n"
     ]
    }
   ],
   "source": [
    "# Read dataset into a PyArrow Table\n",
    "# For large datasets, use column selection and filtering\n",
    "\n",
    "print(\"Reading data into PyArrow Table...\")\n",
    "\n",
    "# ============================================================================\n",
    "# PARAMETER: Number of days to read from parquet files\n",
    "# ============================================================================\n",
    "# Controls how many days of historical data to load from the partitioned parquet files\n",
    "# - Lower values = faster loading, less data, more recent data only\n",
    "# - Higher values = slower loading, more data, broader historical coverage\n",
    "# Default: 5 days (last 5 days of available data)\n",
    "NUM_DAYS_TO_READ = 5\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"Reading the last {NUM_DAYS_TO_READ} days of data...\\n\")\n",
    "\n",
    "# Get all unique days by scanning partition information\n",
    "import datetime\n",
    "\n",
    "# Get fragments to identify unique dates\n",
    "all_partitions = []\n",
    "for fragment in dataset.get_fragments():\n",
    "    partition_expr = fragment.partition_expression\n",
    "    # Extract year, month, day from partition expression\n",
    "    partition_str = str(partition_expr)\n",
    "    all_partitions.append(partition_str)\n",
    "\n",
    "# Get unique partition combinations and sort them\n",
    "unique_partitions = sorted(set(all_partitions))\n",
    "\n",
    "print(f\"Total unique date partitions available: {len(unique_partitions)}\")\n",
    "print(f\"Last few partitions: {unique_partitions[-min(5, len(unique_partitions)):]}\\n\")\n",
    "\n",
    "# Parse last N partitions to build date filter\n",
    "selected_days = []\n",
    "for partition in unique_partitions[-(NUM_DAYS_TO_READ * 3):]:  # Get extra in case some partitions aren't dates\n",
    "    # Extract year, month, day values from partition string\n",
    "    # Format: \"(((year == 2025) and (month == 01)) and (day == 01))\"\n",
    "    try:\n",
    "        if 'year' in partition and 'month' in partition and 'day' in partition:\n",
    "            year_str = partition.split('year == ')[1].split(')')[0].strip().strip(\"'\\\"\")\n",
    "            month_str = partition.split('month == ')[1].split(')')[0].strip().strip(\"'\\\"\")\n",
    "            day_str = partition.split('day == ')[1].split(')')[0].strip().strip(\"'\\\"\")\n",
    "            \n",
    "            # Convert to integers for proper comparison\n",
    "            year = int(year_str)\n",
    "            month = int(month_str)\n",
    "            day = int(day_str)\n",
    "            \n",
    "            selected_days.append((year, month, day))\n",
    "            if len(selected_days) >= NUM_DAYS_TO_READ:\n",
    "                break\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "print(f\"Selected {len(selected_days)} days:\")\n",
    "for year, month, day in selected_days:\n",
    "    print(f\"  - {year:04d}-{month:02d}-{day:02d}\")\n",
    "\n",
    "# Build filter expression for selected days\n",
    "if selected_days:\n",
    "    # Create OR filter for each day\n",
    "    day_filters = []\n",
    "    for year, month, day in selected_days:\n",
    "        day_filter = (\n",
    "            (pc.field('year') == year) & \n",
    "            (pc.field('month') == month) & \n",
    "            (pc.field('day') == day)\n",
    "        )\n",
    "        day_filters.append(day_filter)\n",
    "    \n",
    "    # Combine all day filters with OR\n",
    "    combined_filter = day_filters[0]\n",
    "    for day_filter in day_filters[1:]:\n",
    "        combined_filter = combined_filter | day_filter\n",
    "    \n",
    "    # Read data with filter - reading ALL columns\n",
    "    sample_table = dataset.to_table(\n",
    "        filter=combined_filter\n",
    "    )\n",
    "else:\n",
    "    # Fallback: just read with a limit\n",
    "    sample_table = dataset.to_table().slice(0, 10000)\n",
    "\n",
    "print(f\"\\nData loaded from last {len(selected_days)} days!\")\n",
    "print(f\"Rows: {sample_table.num_rows:,}\")\n",
    "print(f\"Columns: {sample_table.num_columns}\")\n",
    "print(f\"Size in memory: ~{sample_table.nbytes / 1024 / 1024:.1f} MB\")\n",
    "\n",
    "print(f\"\\nFirst 5 rows (showing ALL {sample_table.num_columns} columns):\")\n",
    "print(sample_table.slice(0, 5).to_pandas())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "509c7c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted to Pandas DataFrame\n",
      "Shape: (1645930, 200)\n",
      "\n",
      "Data types:\n",
      "date                     object\n",
      "serial_number            object\n",
      "model                    object\n",
      "capacity_bytes            int64\n",
      "failure                   int64\n",
      "                         ...   \n",
      "smart_255_normalized    float64\n",
      "smart_255_raw           float64\n",
      "year                      int32\n",
      "month                     int32\n",
      "day                       int32\n",
      "Length: 200, dtype: object\n",
      "\n",
      "First few rows:\n",
      "         date serial_number           model  capacity_bytes  failure  \\\n",
      "0  2025-09-23  2206E608DB42  CT250MX500SSD1    250059350016        0   \n",
      "1  2025-09-23  2207E60CC65A  CT250MX500SSD1    250059350016        0   \n",
      "2  2025-09-23  2340E87B92B5  CT250MX500SSD1    250059350016        0   \n",
      "3  2025-09-23  2340E87B97E8  CT250MX500SSD1    250059350016        0   \n",
      "4  2025-09-23  2407E896B6D5  CT250MX500SSD1    250059350016        0   \n",
      "\n",
      "  datacenter  cluster_id  vault_id  pod_id  pod_slot_num  ...  smart_251_raw  \\\n",
      "0       sac0           0      1028       4           NaN  ...            NaN   \n",
      "1       sac0           0      1028      13           NaN  ...            NaN   \n",
      "2       sac0           0      1028      14           NaN  ...            NaN   \n",
      "3       sac0           0      1028       2           NaN  ...            NaN   \n",
      "4       sac0           0      1028       8           NaN  ...            NaN   \n",
      "\n",
      "   smart_252_normalized  smart_252_raw  smart_254_normalized  smart_254_raw  \\\n",
      "0                   NaN            NaN                   NaN            NaN   \n",
      "1                   NaN            NaN                   NaN            NaN   \n",
      "2                   NaN            NaN                   NaN            NaN   \n",
      "3                   NaN            NaN                   NaN            NaN   \n",
      "4                   NaN            NaN                   NaN            NaN   \n",
      "\n",
      "   smart_255_normalized  smart_255_raw  year  month  day  \n",
      "0                   NaN            NaN  2025      9   23  \n",
      "1                   NaN            NaN  2025      9   23  \n",
      "2                   NaN            NaN  2025      9   23  \n",
      "3                   NaN            NaN  2025      9   23  \n",
      "4                   NaN            NaN  2025      9   23  \n",
      "\n",
      "[5 rows x 200 columns]\n"
     ]
    }
   ],
   "source": [
    "# Convert to Pandas for analysis\n",
    "import pandas as pd\n",
    "\n",
    "df_pandas = sample_table.to_pandas()\n",
    "\n",
    "print(f\"Converted to Pandas DataFrame\")\n",
    "print(f\"Shape: {df_pandas.shape}\")\n",
    "print(f\"\\nData types:\")\n",
    "print(df_pandas.dtypes)\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df_pandas.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57f492e",
   "metadata": {},
   "source": [
    "## PyArrow Dataset Analysis Examples\n",
    "\n",
    "Perform analysis on the Backblaze data using PyArrow and Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "687cc58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating failure rates by manufacturer...\n",
      "\n",
      "Failure rates by manufacturer (January 2025):\n",
      "               total_records  total_failures  failure_rate_pct\n",
      "manufacturer                                                  \n",
      "TOSHIBA               549909              50              0.01\n",
      "WDC                   383069              11              0.00\n",
      "ST16000NM001G         170239               4              0.00\n",
      "HGST                  136048              27              0.02\n",
      "ST12000NM0008          93507               9              0.01\n",
      "ST8000NM0055           66427              10              0.02\n",
      "ST12000NM001G          66237               7              0.01\n",
      "ST14000NM001G          52797               3              0.01\n",
      "ST8000DM002            44732               1              0.00\n",
      "ST24000NM002H          36002               2              0.01\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Analyze failure rates by manufacturer using Pandas\n",
    "print(\"Calculating failure rates by manufacturer...\\n\")\n",
    "\n",
    "# Extract manufacturer from model\n",
    "df_pandas['manufacturer'] = df_pandas['model'].str.split(' ').str[0]\n",
    "\n",
    "# Group and calculate statistics\n",
    "manufacturer_stats = df_pandas.groupby('manufacturer').agg({\n",
    "    'failure': ['count', 'sum']\n",
    "})\n",
    "\n",
    "manufacturer_stats.columns = ['total_records', 'total_failures']\n",
    "manufacturer_stats['failure_rate_pct'] = (\n",
    "    manufacturer_stats['total_failures'] / manufacturer_stats['total_records'] * 100\n",
    ").round(2)\n",
    "\n",
    "manufacturer_stats = manufacturer_stats.sort_values('total_records', ascending=False)\n",
    "\n",
    "print(\"Failure rates by manufacturer (January 2025):\")\n",
    "print(manufacturer_stats.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c54058",
   "metadata": {},
   "source": [
    "## Additional Glue Operations\n",
    "\n",
    "### Convert to Pandas (for small datasets)\n",
    "```python\n",
    "# Only convert small subsets to Pandas to avoid memory issues\n",
    "pandas_df = df_spark.limit(1000).toPandas()\n",
    "```\n",
    "\n",
    "### Write processed data back to S3\n",
    "```python\n",
    "# Write as Parquet\n",
    "df_spark.write.mode(\"overwrite\").parquet(\"s3://bucket/path/output/\")\n",
    "\n",
    "# Write as CSV\n",
    "df_spark.write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"s3://bucket/path/output/\")\n",
    "```\n",
    "\n",
    "### Use Glue Data Catalog\n",
    "```python\n",
    "# Write to Glue Data Catalog table\n",
    "glueContext.write_dynamic_frame.from_catalog(\n",
    "    frame=datasource,\n",
    "    database=\"backblaze_db\",\n",
    "    table_name=\"drive_stats\"\n",
    ")\n",
    "```\n",
    "\n",
    "### Partition output data\n",
    "```python\n",
    "# Write with partitioning\n",
    "df_spark.write.mode(\"overwrite\") \\\n",
    "    .partitionBy(\"year\", \"month\") \\\n",
    "    .parquet(\"s3://bucket/path/output/\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0c41d7",
   "metadata": {},
   "source": [
    "## Complete Column Information\n",
    "\n",
    "Display detailed information about all columns in the Backblaze dataset including data types, descriptions, and sample values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "528b030f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COMPLETE COLUMN INFORMATION - BACKBLAZE DATASET\n",
      "================================================================================\n",
      "\n",
      "Total Columns: 200\n",
      "\n",
      "All Columns with Data Types:\n",
      "--------------------------------------------------------------------------------\n",
      " Index          Column Name    Data Type  Nullable\n",
      "     1                 date large_string      True\n",
      "     2        serial_number large_string      True\n",
      "     3                model large_string      True\n",
      "     4       capacity_bytes        int64      True\n",
      "     5              failure        int64      True\n",
      "     6           datacenter large_string      True\n",
      "     7           cluster_id        int64      True\n",
      "     8             vault_id        int64      True\n",
      "     9               pod_id        int64      True\n",
      "    10         pod_slot_num       double      True\n",
      "    11     is_legacy_format         bool      True\n",
      "    12   smart_1_normalized       double      True\n",
      "    13          smart_1_raw       double      True\n",
      "    14   smart_2_normalized       double      True\n",
      "    15          smart_2_raw       double      True\n",
      "    16   smart_3_normalized       double      True\n",
      "    17          smart_3_raw       double      True\n",
      "    18   smart_4_normalized       double      True\n",
      "    19          smart_4_raw       double      True\n",
      "    20   smart_5_normalized       double      True\n",
      "    21          smart_5_raw       double      True\n",
      "    22   smart_7_normalized       double      True\n",
      "    23          smart_7_raw       double      True\n",
      "    24   smart_8_normalized       double      True\n",
      "    25          smart_8_raw       double      True\n",
      "    26   smart_9_normalized       double      True\n",
      "    27          smart_9_raw       double      True\n",
      "    28  smart_10_normalized       double      True\n",
      "    29         smart_10_raw       double      True\n",
      "    30  smart_11_normalized       double      True\n",
      "    31         smart_11_raw       double      True\n",
      "    32  smart_12_normalized       double      True\n",
      "    33         smart_12_raw       double      True\n",
      "    34  smart_13_normalized       double      True\n",
      "    35         smart_13_raw       double      True\n",
      "    36  smart_15_normalized       double      True\n",
      "    37         smart_15_raw       double      True\n",
      "    38  smart_16_normalized       double      True\n",
      "    39         smart_16_raw       double      True\n",
      "    40  smart_17_normalized       double      True\n",
      "    41         smart_17_raw       double      True\n",
      "    42  smart_18_normalized       double      True\n",
      "    43         smart_18_raw       double      True\n",
      "    44  smart_22_normalized       double      True\n",
      "    45         smart_22_raw       double      True\n",
      "    46  smart_23_normalized       double      True\n",
      "    47         smart_23_raw       double      True\n",
      "    48  smart_24_normalized       double      True\n",
      "    49         smart_24_raw       double      True\n",
      "    50  smart_27_normalized       double      True\n",
      "    51         smart_27_raw       double      True\n",
      "    52  smart_71_normalized       double      True\n",
      "    53         smart_71_raw       double      True\n",
      "    54  smart_82_normalized       double      True\n",
      "    55         smart_82_raw       double      True\n",
      "    56  smart_90_normalized       double      True\n",
      "    57         smart_90_raw       double      True\n",
      "    58 smart_160_normalized       double      True\n",
      "    59        smart_160_raw       double      True\n",
      "    60 smart_161_normalized       double      True\n",
      "    61        smart_161_raw       double      True\n",
      "    62 smart_163_normalized       double      True\n",
      "    63        smart_163_raw       double      True\n",
      "    64 smart_164_normalized       double      True\n",
      "    65        smart_164_raw       double      True\n",
      "    66 smart_165_normalized       double      True\n",
      "    67        smart_165_raw       double      True\n",
      "    68 smart_166_normalized       double      True\n",
      "    69        smart_166_raw       double      True\n",
      "    70 smart_167_normalized       double      True\n",
      "    71        smart_167_raw       double      True\n",
      "    72 smart_168_normalized       double      True\n",
      "    73        smart_168_raw       double      True\n",
      "    74 smart_169_normalized       double      True\n",
      "    75        smart_169_raw       double      True\n",
      "    76 smart_170_normalized       double      True\n",
      "    77        smart_170_raw       double      True\n",
      "    78 smart_171_normalized       double      True\n",
      "    79        smart_171_raw       double      True\n",
      "    80 smart_172_normalized       double      True\n",
      "    81        smart_172_raw       double      True\n",
      "    82 smart_173_normalized       double      True\n",
      "    83        smart_173_raw       double      True\n",
      "    84 smart_174_normalized       double      True\n",
      "    85        smart_174_raw       double      True\n",
      "    86 smart_175_normalized       double      True\n",
      "    87        smart_175_raw       double      True\n",
      "    88 smart_176_normalized       double      True\n",
      "    89        smart_176_raw       double      True\n",
      "    90 smart_177_normalized       double      True\n",
      "    91        smart_177_raw       double      True\n",
      "    92 smart_178_normalized       double      True\n",
      "    93        smart_178_raw       double      True\n",
      "    94 smart_179_normalized       double      True\n",
      "    95        smart_179_raw       double      True\n",
      "    96 smart_180_normalized       double      True\n",
      "    97        smart_180_raw       double      True\n",
      "    98 smart_181_normalized       double      True\n",
      "    99        smart_181_raw       double      True\n",
      "   100 smart_182_normalized       double      True\n",
      "   101        smart_182_raw       double      True\n",
      "   102 smart_183_normalized       double      True\n",
      "   103        smart_183_raw       double      True\n",
      "   104 smart_184_normalized       double      True\n",
      "   105        smart_184_raw       double      True\n",
      "   106 smart_187_normalized       double      True\n",
      "   107        smart_187_raw       double      True\n",
      "   108 smart_188_normalized       double      True\n",
      "   109        smart_188_raw       double      True\n",
      "   110 smart_189_normalized       double      True\n",
      "   111        smart_189_raw       double      True\n",
      "   112 smart_190_normalized       double      True\n",
      "   113        smart_190_raw       double      True\n",
      "   114 smart_191_normalized       double      True\n",
      "   115        smart_191_raw       double      True\n",
      "   116 smart_192_normalized       double      True\n",
      "   117        smart_192_raw       double      True\n",
      "   118 smart_193_normalized       double      True\n",
      "   119        smart_193_raw       double      True\n",
      "   120 smart_194_normalized       double      True\n",
      "   121        smart_194_raw       double      True\n",
      "   122 smart_195_normalized       double      True\n",
      "   123        smart_195_raw       double      True\n",
      "   124 smart_196_normalized       double      True\n",
      "   125        smart_196_raw       double      True\n",
      "   126 smart_197_normalized       double      True\n",
      "   127        smart_197_raw       double      True\n",
      "   128 smart_198_normalized       double      True\n",
      "   129        smart_198_raw       double      True\n",
      "   130 smart_199_normalized       double      True\n",
      "   131        smart_199_raw       double      True\n",
      "   132 smart_200_normalized       double      True\n",
      "   133        smart_200_raw       double      True\n",
      "   134 smart_201_normalized       double      True\n",
      "   135        smart_201_raw       double      True\n",
      "   136 smart_202_normalized       double      True\n",
      "   137        smart_202_raw       double      True\n",
      "   138 smart_206_normalized       double      True\n",
      "   139        smart_206_raw       double      True\n",
      "   140 smart_210_normalized       double      True\n",
      "   141        smart_210_raw       double      True\n",
      "   142 smart_211_normalized       double      True\n",
      "   143        smart_211_raw       double      True\n",
      "   144 smart_212_normalized       double      True\n",
      "   145        smart_212_raw       double      True\n",
      "   146 smart_218_normalized       double      True\n",
      "   147        smart_218_raw       double      True\n",
      "   148 smart_220_normalized       double      True\n",
      "   149        smart_220_raw       double      True\n",
      "   150 smart_222_normalized       double      True\n",
      "   151        smart_222_raw       double      True\n",
      "   152 smart_223_normalized       double      True\n",
      "   153        smart_223_raw       double      True\n",
      "   154 smart_224_normalized       double      True\n",
      "   155        smart_224_raw       double      True\n",
      "   156 smart_225_normalized       double      True\n",
      "   157        smart_225_raw       double      True\n",
      "   158 smart_226_normalized       double      True\n",
      "   159        smart_226_raw       double      True\n",
      "   160 smart_230_normalized       double      True\n",
      "   161        smart_230_raw       double      True\n",
      "   162 smart_231_normalized       double      True\n",
      "   163        smart_231_raw       double      True\n",
      "   164 smart_232_normalized       double      True\n",
      "   165        smart_232_raw       double      True\n",
      "   166 smart_233_normalized       double      True\n",
      "   167        smart_233_raw       double      True\n",
      "   168 smart_234_normalized       double      True\n",
      "   169        smart_234_raw       double      True\n",
      "   170 smart_235_normalized       double      True\n",
      "   171        smart_235_raw       double      True\n",
      "   172 smart_240_normalized       double      True\n",
      "   173        smart_240_raw       double      True\n",
      "   174 smart_241_normalized       double      True\n",
      "   175        smart_241_raw       double      True\n",
      "   176 smart_242_normalized       double      True\n",
      "   177        smart_242_raw       double      True\n",
      "   178 smart_244_normalized       double      True\n",
      "   179        smart_244_raw       double      True\n",
      "   180 smart_245_normalized       double      True\n",
      "   181        smart_245_raw       double      True\n",
      "   182 smart_246_normalized       double      True\n",
      "   183        smart_246_raw       double      True\n",
      "   184 smart_247_normalized       double      True\n",
      "   185        smart_247_raw       double      True\n",
      "   186 smart_248_normalized       double      True\n",
      "   187        smart_248_raw       double      True\n",
      "   188 smart_250_normalized       double      True\n",
      "   189        smart_250_raw       double      True\n",
      "   190 smart_251_normalized       double      True\n",
      "   191        smart_251_raw       double      True\n",
      "   192 smart_252_normalized       double      True\n",
      "   193        smart_252_raw       double      True\n",
      "   194 smart_254_normalized       double      True\n",
      "   195        smart_254_raw       double      True\n",
      "   196 smart_255_normalized       double      True\n",
      "   197        smart_255_raw       double      True\n",
      "   198                 year        int32      True\n",
      "   199                month        int32      True\n",
      "   200                  day        int32      True\n"
     ]
    }
   ],
   "source": [
    "# Get complete schema information using PyArrow\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"COMPLETE COLUMN INFORMATION - BACKBLAZE DATASET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "schema_fields = list(dataset.schema)\n",
    "print(f\"\\nTotal Columns: {len(schema_fields)}\\n\")\n",
    "\n",
    "# Create a detailed column info dataframe\n",
    "column_info = []\n",
    "for i, field in enumerate(schema_fields, 1):\n",
    "    column_info.append({\n",
    "        'Index': i,\n",
    "        'Column Name': field.name,\n",
    "        'Data Type': str(field.type),\n",
    "        'Nullable': field.nullable\n",
    "    })\n",
    "\n",
    "df_columns = pd.DataFrame(column_info)\n",
    "\n",
    "# Display all columns\n",
    "print(\"All Columns with Data Types:\")\n",
    "print(\"-\"*80)\n",
    "pd.set_option('display.max_rows', None)\n",
    "print(df_columns.to_string(index=False))\n",
    "pd.reset_option('display.max_rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2cd2e8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "COLUMNS BY CATEGORY\n",
      "================================================================================\n",
      "\n",
      "1. CORE DRIVE IDENTIFICATION (5 columns):\n",
      "   • date: large_string\n",
      "   • serial_number: large_string\n",
      "   • model: large_string\n",
      "   • capacity_bytes: int64\n",
      "   • failure: int64\n",
      "\n",
      "2. DATACENTER LOCATION (5 columns):\n",
      "   • datacenter: large_string\n",
      "   • cluster_id: int64\n",
      "   • vault_id: int64\n",
      "   • pod_id: int64\n",
      "   • pod_slot_num: double\n",
      "\n",
      "3. PARTITION KEYS (3 columns):\n",
      "   • year: int32\n",
      "   • month: int32\n",
      "   • day: int32\n",
      "\n",
      "4. SMART ATTRIBUTES (186 columns):\n",
      "   These are hard drive health monitoring attributes\n",
      "   Format: smart_X_normalized and smart_X_raw\n",
      "\n",
      "   Common SMART attributes include:\n",
      "   SMART IDs present: [1, 2, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 15, 16, 17, 18, 22, 23, 24, 27]...\n",
      "   Total unique SMART IDs: 93\n"
     ]
    }
   ],
   "source": [
    "# Categorize columns by type\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COLUMNS BY CATEGORY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Core identification columns\n",
    "core_cols = [f for f in schema_fields if f.name in ['date', 'serial_number', 'model', 'capacity_bytes', 'failure']]\n",
    "print(f\"\\n1. CORE DRIVE IDENTIFICATION ({len(core_cols)} columns):\")\n",
    "for field in core_cols:\n",
    "    print(f\"   • {field.name}: {field.type}\")\n",
    "\n",
    "# Location/datacenter columns\n",
    "location_cols = [f for f in schema_fields if f.name in ['datacenter', 'cluster_id', 'vault_id', 'pod_id', 'pod_slot_num']]\n",
    "print(f\"\\n2. DATACENTER LOCATION ({len(location_cols)} columns):\")\n",
    "for field in location_cols:\n",
    "    print(f\"   • {field.name}: {field.type}\")\n",
    "\n",
    "# Partition columns\n",
    "partition_cols = [f for f in schema_fields if f.name in ['year', 'month', 'day']]\n",
    "print(f\"\\n3. PARTITION KEYS ({len(partition_cols)} columns):\")\n",
    "for field in partition_cols:\n",
    "    print(f\"   • {field.name}: {field.type}\")\n",
    "\n",
    "# SMART attribute columns\n",
    "smart_cols = [f for f in schema_fields if 'smart' in f.name.lower()]\n",
    "print(f\"\\n4. SMART ATTRIBUTES ({len(smart_cols)} columns):\")\n",
    "print(f\"   These are hard drive health monitoring attributes\")\n",
    "print(f\"   Format: smart_X_normalized and smart_X_raw\")\n",
    "print(f\"\\n   Common SMART attributes include:\")\n",
    "\n",
    "# Group SMART attributes by ID\n",
    "smart_ids = set()\n",
    "for field in smart_cols:\n",
    "    if 'smart_' in field.name:\n",
    "        parts = field.name.split('_')\n",
    "        if len(parts) >= 2 and parts[1].isdigit():\n",
    "            smart_ids.add(int(parts[1]))\n",
    "\n",
    "smart_ids = sorted(list(smart_ids))\n",
    "print(f\"   SMART IDs present: {smart_ids[:20]}...\")\n",
    "print(f\"   Total unique SMART IDs: {len(smart_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f215da30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "KEY SMART ATTRIBUTES - DETAILED DESCRIPTIONS\n",
      "================================================================================\n",
      "\n",
      "CRITICAL FAILURE INDICATORS:\n",
      "--------------------------------------------------------------------------------\n",
      "SMART   5: Reallocated Sectors Count - Count of reallocated sectors (CRITICAL)\n",
      "SMART 187: Reported Uncorrectable Errors - Errors that could not be corrected (CRITICAL)\n",
      "SMART 197: Current Pending Sector Count - Sectors waiting to be remapped (CRITICAL)\n",
      "SMART 198: Offline Uncorrectable - Uncorrectable sector count (CRITICAL)\n",
      "\n",
      "COMMONLY MONITORED ATTRIBUTES:\n",
      "--------------------------------------------------------------------------------\n",
      "SMART   1: Read Error Rate - Rate of hardware read errors\n",
      "SMART   3: Spin-Up Time - Time to spin up the drive\n",
      "SMART   4: Start/Stop Count - Number of drive start/stop cycles\n",
      "SMART   7: Seek Error Rate - Rate of seek errors\n",
      "SMART   9: Power-On Hours - Total hours drive has been powered on\n",
      "SMART  12: Power Cycle Count - Number of power-on/off cycles\n",
      "SMART 190: Temperature - Drive temperature in Celsius\n",
      "SMART 194: Temperature Celsius - Current drive temperature\n",
      "SMART 199: UltraDMA CRC Error Count - CRC errors during data transfer\n",
      "SMART 241: Total LBAs Written - Lifetime total data written\n",
      "SMART 242: Total LBAs Read - Lifetime total data read\n",
      "\n",
      "Note: Each SMART attribute has two values:\n",
      "  • smart_X_normalized: Normalized value (0-100, higher is usually better)\n",
      "  • smart_X_raw: Raw value (actual count/measurement)\n"
     ]
    }
   ],
   "source": [
    "# Detailed SMART attribute descriptions\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"KEY SMART ATTRIBUTES - DETAILED DESCRIPTIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "smart_descriptions = {\n",
    "    1: \"Read Error Rate - Rate of hardware read errors\",\n",
    "    2: \"Throughput Performance - Overall throughput performance\",\n",
    "    3: \"Spin-Up Time - Time to spin up the drive\",\n",
    "    4: \"Start/Stop Count - Number of drive start/stop cycles\",\n",
    "    5: \"Reallocated Sectors Count - Count of reallocated sectors (CRITICAL)\",\n",
    "    7: \"Seek Error Rate - Rate of seek errors\",\n",
    "    8: \"Seek Time Performance - Average seek time performance\",\n",
    "    9: \"Power-On Hours - Total hours drive has been powered on\",\n",
    "    10: \"Spin Retry Count - Number of retry attempts to spin up\",\n",
    "    11: \"Calibration Retry Count - Number of calibration retries\",\n",
    "    12: \"Power Cycle Count - Number of power-on/off cycles\",\n",
    "    183: \"Runtime Bad Block - Runtime bad block count\",\n",
    "    184: \"End-to-End Error - End-to-end error detection count\",\n",
    "    187: \"Reported Uncorrectable Errors - Errors that could not be corrected (CRITICAL)\",\n",
    "    188: \"Command Timeout - Count of command timeout events\",\n",
    "    189: \"High Fly Writes - Head flying height errors\",\n",
    "    190: \"Temperature - Drive temperature in Celsius\",\n",
    "    191: \"G-Sense Error Rate - Rate of errors from shock/vibration\",\n",
    "    192: \"Power-Off Retract Count - Emergency head retracts\",\n",
    "    193: \"Load/Unload Cycle Count - Number of head load/unload cycles\",\n",
    "    194: \"Temperature Celsius - Current drive temperature\",\n",
    "    195: \"Hardware ECC Recovered - Errors corrected by ECC\",\n",
    "    196: \"Reallocation Event Count - Count of remap operations\",\n",
    "    197: \"Current Pending Sector Count - Sectors waiting to be remapped (CRITICAL)\",\n",
    "    198: \"Offline Uncorrectable - Uncorrectable sector count (CRITICAL)\",\n",
    "    199: \"UltraDMA CRC Error Count - CRC errors during data transfer\",\n",
    "    200: \"Multi-Zone Error Rate - Write error rate\",\n",
    "    220: \"Disk Shift - Drive shift from intended position\",\n",
    "    222: \"Loaded Hours - Time spent with heads loaded\",\n",
    "    223: \"Load/Unload Retry Count - Head load/unload retry attempts\",\n",
    "    224: \"Load Friction - Friction during head load\",\n",
    "    225: \"Load/Unload Cycle Count - Total head load cycles\",\n",
    "    226: \"Load In Time - Time to load heads\",\n",
    "    240: \"Head Flying Hours - Time spent with heads flying\",\n",
    "    241: \"Total LBAs Written - Lifetime total data written\",\n",
    "    242: \"Total LBAs Read - Lifetime total data read\"\n",
    "}\n",
    "\n",
    "print(\"\\nCRITICAL FAILURE INDICATORS:\")\n",
    "print(\"-\"*80)\n",
    "critical_smart = [5, 187, 197, 198]\n",
    "for smart_id in critical_smart:\n",
    "    if smart_id in smart_descriptions:\n",
    "        print(f\"SMART {smart_id:3d}: {smart_descriptions[smart_id]}\")\n",
    "\n",
    "print(\"\\nCOMMONLY MONITORED ATTRIBUTES:\")\n",
    "print(\"-\"*80)\n",
    "common_smart = [1, 3, 4, 7, 9, 12, 190, 194, 199, 241, 242]\n",
    "for smart_id in common_smart:\n",
    "    if smart_id in smart_descriptions:\n",
    "        print(f\"SMART {smart_id:3d}: {smart_descriptions[smart_id]}\")\n",
    "\n",
    "print(f\"\\nNote: Each SMART attribute has two values:\")\n",
    "print(f\"  • smart_X_normalized: Normalized value (0-100, higher is usually better)\")\n",
    "print(f\"  • smart_X_raw: Raw value (actual count/measurement)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c48ded",
   "metadata": {},
   "source": [
    "## Read All Review Parquet Files into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4b6030d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 parquet files\n",
      "No parquet files found. Checking if files exist from S3...\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Get all parquet files from the reviews parquet directory\n",
    "parquet_dir = \"amazon_reviews_download/parquet/Electronics\"\n",
    "parquet_pattern = os.path.join(parquet_dir, \"**/*.parquet\")\n",
    "all_parquet_files = glob.glob(parquet_pattern, recursive=True)\n",
    "\n",
    "print(f\"Found {len(all_parquet_files)} parquet files\")\n",
    "if all_parquet_files:\n",
    "    print(f\"First few files: {all_parquet_files[:5]}\")\n",
    "    \n",
    "    # Read all parquet files into a single dataframe\n",
    "    df_all_reviews = pd.concat([pd.read_parquet(f) for f in all_parquet_files], ignore_index=True)\n",
    "    \n",
    "    print(f\"\\nDataFrame shape: {df_all_reviews.shape}\")\n",
    "    print(f\"Columns: {list(df_all_reviews.columns)}\")\n",
    "    print(f\"\\nFirst few rows:\")\n",
    "    print(df_all_reviews.head())\n",
    "else:\n",
    "    print(\"No parquet files found. Checking if files exist from S3...\")\n",
    "    # If no local files, use the reviews_parquet_files list from S3\n",
    "    if 'reviews_parquet_files' in dir() and reviews_parquet_files:\n",
    "        print(f\"Found {len(reviews_parquet_files)} parquet files in S3\")\n",
    "        print(f\"First few files: {reviews_parquet_files[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "677bb44e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 1 parquet file(s) from S3...\n",
      "\n",
      "[1/1] Reading: s3://mlops-backblaze-d7b30cb5-us-east-1/raw/reviews_2023_parquet/raw_review_Electronics/part-000000.parquet\n",
      "  Shape: (171573, 10)\n",
      "\n",
      "============================================================\n",
      "Combined DataFrame shape: (171573, 10)\n",
      "Columns: ['rating', 'title', 'text', 'images', 'asin', 'parent_asin', 'user_id', 'timestamp', 'helpful_vote', 'verified_purchase']\n",
      "\n",
      "First few rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>images</th>\n",
       "      <th>asin</th>\n",
       "      <th>parent_asin</th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>helpful_vote</th>\n",
       "      <th>verified_purchase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>Great solid flash drive - Toshiba quality and ...</td>\n",
       "      <td>Not much can be said that hasn't been said in ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>B00IEZGU6G</td>\n",
       "      <td>B00WHEUS22</td>\n",
       "      <td>AFSKPY37N3C43SOI5IEXEK5JSIYA</td>\n",
       "      <td>1411650573000</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>GREAT External Drive</td>\n",
       "      <td>great price  -  I already had a Seagate 4TB an...</td>\n",
       "      <td>[]</td>\n",
       "      <td>B01M00UHV8</td>\n",
       "      <td>B07454F4JH</td>\n",
       "      <td>AHGAOIZVODNHYMNCBV4DECZH42UQ</td>\n",
       "      <td>1494691925000</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>Long Term Life is Questionable; Seagate not go...</td>\n",
       "      <td>The first one of these I got was DOA, but Seag...</td>\n",
       "      <td>[]</td>\n",
       "      <td>B01IEKG3TY</td>\n",
       "      <td>B01IEKG3TY</td>\n",
       "      <td>AFAIJYOUO3NAWLBDIKTQSC3DASWA</td>\n",
       "      <td>1472494627000</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>Compact Attractive, Quiet Unit</td>\n",
       "      <td>Several reviewers mentioned the difficulty in ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>B002HKEQZ6</td>\n",
       "      <td>B002OB4DC4</td>\n",
       "      <td>AFAIJYOUO3NAWLBDIKTQSC3DASWA</td>\n",
       "      <td>1306211962000</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Works perfectly</td>\n",
       "      <td>I had a WD blue M.2 SSD that kept dropping off...</td>\n",
       "      <td>[]</td>\n",
       "      <td>B09JCD2CN6</td>\n",
       "      <td>B09JCD2CN6</td>\n",
       "      <td>AFJBKPK5W56XWSNPQU2WW66ISWYQ</td>\n",
       "      <td>1640360340432</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating                                              title  \\\n",
       "0     4.0  Great solid flash drive - Toshiba quality and ...   \n",
       "1     5.0                               GREAT External Drive   \n",
       "2     3.0  Long Term Life is Questionable; Seagate not go...   \n",
       "3     4.0                     Compact Attractive, Quiet Unit   \n",
       "4     5.0                                    Works perfectly   \n",
       "\n",
       "                                                text images        asin  \\\n",
       "0  Not much can be said that hasn't been said in ...     []  B00IEZGU6G   \n",
       "1  great price  -  I already had a Seagate 4TB an...     []  B01M00UHV8   \n",
       "2  The first one of these I got was DOA, but Seag...     []  B01IEKG3TY   \n",
       "3  Several reviewers mentioned the difficulty in ...     []  B002HKEQZ6   \n",
       "4  I had a WD blue M.2 SSD that kept dropping off...     []  B09JCD2CN6   \n",
       "\n",
       "  parent_asin                       user_id      timestamp  helpful_vote  \\\n",
       "0  B00WHEUS22  AFSKPY37N3C43SOI5IEXEK5JSIYA  1411650573000             0   \n",
       "1  B07454F4JH  AHGAOIZVODNHYMNCBV4DECZH42UQ  1494691925000             1   \n",
       "2  B01IEKG3TY  AFAIJYOUO3NAWLBDIKTQSC3DASWA  1472494627000             1   \n",
       "3  B002OB4DC4  AFAIJYOUO3NAWLBDIKTQSC3DASWA  1306211962000             1   \n",
       "4  B09JCD2CN6  AFJBKPK5W56XWSNPQU2WW66ISWYQ  1640360340432             0   \n",
       "\n",
       "   verified_purchase  \n",
       "0              False  \n",
       "1               True  \n",
       "2              False  \n",
       "3              False  \n",
       "4              False  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 171573 entries, 0 to 171572\n",
      "Data columns (total 10 columns):\n",
      " #   Column             Non-Null Count   Dtype  \n",
      "---  ------             --------------   -----  \n",
      " 0   rating             171573 non-null  float64\n",
      " 1   title              171573 non-null  object \n",
      " 2   text               171573 non-null  object \n",
      " 3   images             171573 non-null  object \n",
      " 4   asin               171573 non-null  object \n",
      " 5   parent_asin        171573 non-null  object \n",
      " 6   user_id            171573 non-null  object \n",
      " 7   timestamp          171573 non-null  int64  \n",
      " 8   helpful_vote       171573 non-null  int64  \n",
      " 9   verified_purchase  171573 non-null  bool   \n",
      "dtypes: bool(1), float64(1), int64(2), object(6)\n",
      "memory usage: 11.9+ MB\n"
     ]
    }
   ],
   "source": [
    "# Read all parquet files from S3\n",
    "df_all_reviews_s3 = pd.DataFrame()\n",
    "\n",
    "# List parquet files from S3\n",
    "reviews_prefix = \"raw/reviews_2023_parquet/raw_review_Electronics/\"\n",
    "response = s3.list_objects_v2(Bucket=bucket, Prefix=reviews_prefix)\n",
    "reviews_parquet_files = [obj['Key'] for obj in response.get('Contents', []) if obj['Key'].endswith('.parquet')]\n",
    "\n",
    "if reviews_parquet_files:\n",
    "    print(f\"Reading {len(reviews_parquet_files)} parquet file(s) from S3...\")\n",
    "    \n",
    "    for i, parquet_path in enumerate(reviews_parquet_files):\n",
    "        full_s3_path = f\"s3://{bucket}/{parquet_path}\"\n",
    "        print(f\"\\n[{i+1}/{len(reviews_parquet_files)}] Reading: {full_s3_path}\")\n",
    "        \n",
    "        # Read parquet file from S3\n",
    "        df_temp = pd.read_parquet(full_s3_path)\n",
    "        print(f\"  Shape: {df_temp.shape}\")\n",
    "        \n",
    "        # Append to main dataframe\n",
    "        df_all_reviews_s3 = pd.concat([df_all_reviews_s3, df_temp], ignore_index=True)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Combined DataFrame shape: {df_all_reviews_s3.shape}\")\n",
    "    print(f\"Columns: {list(df_all_reviews_s3.columns)}\")\n",
    "    print(f\"\\nFirst few rows:\")\n",
    "    display(df_all_reviews_s3.head())\n",
    "    \n",
    "    print(f\"\\nDataFrame info:\")\n",
    "    df_all_reviews_s3.info()\n",
    "else:\n",
    "    print(\"No parquet files available to read.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f223553",
   "metadata": {},
   "source": [
    "## Apply Join Columns - Extract Manufacturer and Model Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "41d221ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting manufacturer from review titles...\n",
      "Filling missing manufacturers from review text...\n",
      "Extracting model hints from titles...\n",
      "Filling missing model hints from review text...\n",
      "\n",
      "================================================================================\n",
      "Reviews with manufacturer extracted:\n",
      "  parent_asin                                              title manufacturer  \\\n",
      "0  B00WHEUS22  Great solid flash drive - Toshiba quality and ...      TOSHIBA   \n",
      "1  B07454F4JH                               GREAT External Drive      SEAGATE   \n",
      "2  B01IEKG3TY  Long Term Life is Questionable; Seagate not go...      SEAGATE   \n",
      "3  B002OB4DC4                     Compact Attractive, Quiet Unit      SEAGATE   \n",
      "4  B09JCD2CN6                                    Works perfectly          WDC   \n",
      "5  B0C6QYPNYR                               Great for mobile use          WDC   \n",
      "6  B01FRP1ZHE                           Chic looking hard drive.          WDC   \n",
      "7  B01BCWKBZI                                    Fast and small.          WDC   \n",
      "8  B09VS4V18K                                                DOA          WDC   \n",
      "9  B018AX3OHO  This thing is even faster than my Macbook than...          WDC   \n",
      "\n",
      "                                         model_hints  \n",
      "0                  [TOSHIBA QUALITY, AND DURABILITY]  \n",
      "1                                   [GREAT EXTERNAL]  \n",
      "2                    [IS QUESTIONABLE, GOOD QUALITY]  \n",
      "3                               [COMPACT ATTRACTIVE]  \n",
      "4                                  [WORKS PERFECTLY]  \n",
      "5                                       [FOR MOBILE]  \n",
      "6                                     [CHIC LOOKING]  \n",
      "7  [TO TRANSFER, OLDER DESKTOP, PCI EXPRESS, GET ...  \n",
      "8  [WD DRIVES, NO PROBLEMS, ONE ARRIVED, BE RETUR...  \n",
      "9                          [EVEN FASTER, MY MACBOOK]  \n",
      "\n",
      "Reviews with manufacturer identified: 163,406 / 171,573\n",
      "\n",
      "================================================================================\n",
      "Manufacturer distribution in reviews:\n",
      "manufacturer\n",
      "WDC        103758\n",
      "SEAGATE     40251\n",
      "TOSHIBA     16134\n",
      "HGST         3263\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def extract_manufacturer(text):\n",
    "    \"\"\"Extract manufacturer from review text\"\"\"\n",
    "    text_lower = str(text).lower()\n",
    "    \n",
    "    # Map various manufacturer names to standard format\n",
    "    if 'toshiba' in text_lower:\n",
    "        return 'TOSHIBA'\n",
    "    elif 'seagate' in text_lower or 'st' in text_lower[:3]:\n",
    "        return 'SEAGATE'\n",
    "    elif 'western digital' in text_lower or 'wd' in text_lower or 'wdc' in text_lower:\n",
    "        return 'WDC'\n",
    "    elif 'hitachi' in text_lower or 'hgst' in text_lower:\n",
    "        return 'HGST'\n",
    "    \n",
    "    return None\n",
    "\n",
    "def extract_model_hints(text):\n",
    "    \"\"\"Extract potential model numbers/patterns from review text\"\"\"\n",
    "    text_upper = str(text).upper()\n",
    "    \n",
    "    # Look for common model patterns\n",
    "    patterns = [\n",
    "        r'[A-Z]{2,}\\s*[A-Z0-9]{6,}',  # e.g., \"MG08ACA16TA\", \"WUH722222ALE6L4\"\n",
    "        r'ST\\d{4,}[A-Z]{2}\\d{3,}[A-Z]?',  # Seagate pattern\n",
    "        r'WD[A-Z0-9]{6,}',  # WD pattern\n",
    "        r'MG\\d{2}[A-Z]{3}\\d{2}[A-Z]{2,}',  # Toshiba pattern\n",
    "    ]\n",
    "    \n",
    "    models = []\n",
    "    for pattern in patterns:\n",
    "        matches = re.findall(pattern, text_upper)\n",
    "        models.extend(matches)\n",
    "    \n",
    "    return models if models else None\n",
    "\n",
    "# Add manufacturer and model hints to the parquet reviews dataframe\n",
    "print(\"Extracting manufacturer from review titles...\")\n",
    "df_all_reviews_s3['manufacturer'] = df_all_reviews_s3['title'].apply(extract_manufacturer)\n",
    "\n",
    "print(\"Filling missing manufacturers from review text...\")\n",
    "df_all_reviews_s3['manufacturer'] = df_all_reviews_s3['manufacturer'].fillna(\n",
    "    df_all_reviews_s3['text'].apply(extract_manufacturer)\n",
    ")\n",
    "\n",
    "print(\"Extracting model hints from titles...\")\n",
    "df_all_reviews_s3['model_hints'] = df_all_reviews_s3['title'].apply(extract_model_hints)\n",
    "\n",
    "print(\"Filling missing model hints from review text...\")\n",
    "df_all_reviews_s3['model_hints'] = df_all_reviews_s3['model_hints'].fillna(\n",
    "    df_all_reviews_s3['text'].apply(extract_model_hints)\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Reviews with manufacturer extracted:\")\n",
    "print(df_all_reviews_s3[['parent_asin', 'title', 'manufacturer', 'model_hints']].head(10))\n",
    "print(f\"\\nReviews with manufacturer identified: {df_all_reviews_s3['manufacturer'].notna().sum():,} / {len(df_all_reviews_s3):,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Manufacturer distribution in reviews:\")\n",
    "print(df_all_reviews_s3['manufacturer'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933eef24",
   "metadata": {},
   "source": [
    "## Analyze Low Rating Distribution by Manufacturer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cee74dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing 163,406 reviews with identified manufacturers\n",
      "\n",
      "================================================================================\n",
      "PERCENTAGE OF LOW RATINGS BY MANUFACTURER\n",
      "================================================================================\n",
      "\n",
      "DataFrame with manufacturer as index:\n",
      "              pct_one_star  pct_two_star\n",
      "manufacturer                            \n",
      "HGST                 15.45          5.46\n",
      "SEAGATE              25.12          7.27\n",
      "TOSHIBA              17.01          6.10\n",
      "WDC                  13.55          5.77\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Detailed breakdown including counts:\n",
      "              total_reviews  one_star_count  two_star_count  pct_one_star  \\\n",
      "manufacturer                                                                \n",
      "WDC                  103758           14061            5985         13.55   \n",
      "SEAGATE               40251           10111            2925         25.12   \n",
      "TOSHIBA               16134            2744             984         17.01   \n",
      "HGST                   3263             504             178         15.45   \n",
      "\n",
      "              pct_two_star  \n",
      "manufacturer                \n",
      "WDC                   5.77  \n",
      "SEAGATE               7.27  \n",
      "TOSHIBA               6.10  \n",
      "HGST                  5.46  \n",
      "\n",
      "================================================================================\n",
      "\n",
      "Key insights:\n",
      "  • Manufacturer with highest % of 1-star reviews: SEAGATE (25.12%)\n",
      "  • Manufacturer with lowest % of 1-star reviews: WDC (13.55%)\n",
      "  • Manufacturer with highest % of 2-star reviews: SEAGATE (7.27%)\n",
      "  • Manufacturer with lowest % of 2-star reviews: HGST (5.46%)\n"
     ]
    }
   ],
   "source": [
    "# Filter to only reviews with identified manufacturers\n",
    "df_with_mfr = df_all_reviews_s3[df_all_reviews_s3['manufacturer'].notna()].copy()\n",
    "\n",
    "print(f\"Analyzing {len(df_with_mfr):,} reviews with identified manufacturers\\n\")\n",
    "\n",
    "# Group by manufacturer and calculate rating distributions\n",
    "mfr_ratings = df_with_mfr.groupby('manufacturer')['rating'].agg([\n",
    "    ('total_reviews', 'count'),\n",
    "    ('one_star_count', lambda x: (x == 1.0).sum()),\n",
    "    ('two_star_count', lambda x: (x == 2.0).sum()),\n",
    "]).reset_index()\n",
    "\n",
    "# Calculate percentages\n",
    "mfr_ratings['pct_one_star'] = (mfr_ratings['one_star_count'] / mfr_ratings['total_reviews'] * 100).round(2)\n",
    "mfr_ratings['pct_two_star'] = (mfr_ratings['two_star_count'] / mfr_ratings['total_reviews'] * 100).round(2)\n",
    "\n",
    "# Create final dataframe with manufacturer as index\n",
    "df_mfr_low_ratings = mfr_ratings[['manufacturer', 'pct_one_star', 'pct_two_star']].set_index('manufacturer')\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PERCENTAGE OF LOW RATINGS BY MANUFACTURER\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nDataFrame with manufacturer as index:\")\n",
    "print(df_mfr_low_ratings)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nDetailed breakdown including counts:\")\n",
    "mfr_ratings_display = mfr_ratings.set_index('manufacturer')\n",
    "print(mfr_ratings_display.sort_values('total_reviews', ascending=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nKey insights:\")\n",
    "print(f\"  • Manufacturer with highest % of 1-star reviews: {df_mfr_low_ratings['pct_one_star'].idxmax()} ({df_mfr_low_ratings['pct_one_star'].max()}%)\")\n",
    "print(f\"  • Manufacturer with lowest % of 1-star reviews: {df_mfr_low_ratings['pct_one_star'].idxmin()} ({df_mfr_low_ratings['pct_one_star'].min()}%)\")\n",
    "print(f\"  • Manufacturer with highest % of 2-star reviews: {df_mfr_low_ratings['pct_two_star'].idxmax()} ({df_mfr_low_ratings['pct_two_star'].max()}%)\")\n",
    "print(f\"  • Manufacturer with lowest % of 2-star reviews: {df_mfr_low_ratings['pct_two_star'].idxmin()} ({df_mfr_low_ratings['pct_two_star'].min()}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebeafb58",
   "metadata": {},
   "source": [
    "## Join Review Features with Backblaze Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b162d39c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardized manufacturer distribution in df_pandas:\n",
      "manufacturer\n",
      "SEAGATE    559901\n",
      "TOSHIBA    549909\n",
      "WDC        383791\n",
      "HGST       136048\n",
      "Name: count, dtype: int64\n",
      "\n",
      "================================================================================\n",
      "JOINED DATAFRAME: BACKBLAZE DATA + REVIEW FEATURES\n",
      "================================================================================\n",
      "\n",
      "Original df_pandas shape: (1645930, 201)\n",
      "Joined df_joined shape: (1645930, 206)\n",
      "New columns added: 5\n",
      "\n",
      "================================================================================\n",
      "New review-based features added to each Backblaze record:\n",
      "  • total_reviews - Total number of reviews for this manufacturer\n",
      "  • one_star_count - Count of 1-star reviews for this manufacturer\n",
      "  • two_star_count - Count of 2-star reviews for this manufacturer\n",
      "  • pct_one_star - Percentage of 1-star reviews (quality indicator)\n",
      "  • pct_two_star - Percentage of 2-star reviews (quality indicator)\n",
      "\n",
      "================================================================================\n",
      "Sample rows with key columns:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>manufacturer</th>\n",
       "      <th>failure</th>\n",
       "      <th>pct_one_star</th>\n",
       "      <th>pct_two_star</th>\n",
       "      <th>total_reviews</th>\n",
       "      <th>capacity_bytes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CT250MX500SSD1</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>250059350016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CT250MX500SSD1</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>250059350016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CT250MX500SSD1</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>250059350016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CT250MX500SSD1</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>250059350016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CT250MX500SSD1</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>250059350016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HGST HUH728080ALE604</td>\n",
       "      <td>HGST</td>\n",
       "      <td>0</td>\n",
       "      <td>15.45</td>\n",
       "      <td>5.46</td>\n",
       "      <td>3263.0</td>\n",
       "      <td>8001563222016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>HGST HUH728080ALE604</td>\n",
       "      <td>HGST</td>\n",
       "      <td>0</td>\n",
       "      <td>15.45</td>\n",
       "      <td>5.46</td>\n",
       "      <td>3263.0</td>\n",
       "      <td>8001563222016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>HGST HUH728080ALE604</td>\n",
       "      <td>HGST</td>\n",
       "      <td>0</td>\n",
       "      <td>15.45</td>\n",
       "      <td>5.46</td>\n",
       "      <td>3263.0</td>\n",
       "      <td>8001563222016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Seagate BarraCuda SSD ZA250CM10002</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>250059350016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ST500LM012 HN</td>\n",
       "      <td>SEAGATE</td>\n",
       "      <td>0</td>\n",
       "      <td>25.12</td>\n",
       "      <td>7.27</td>\n",
       "      <td>40251.0</td>\n",
       "      <td>500107862016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                model manufacturer  failure  pct_one_star  \\\n",
       "0                      CT250MX500SSD1         None        0           NaN   \n",
       "1                      CT250MX500SSD1         None        0           NaN   \n",
       "2                      CT250MX500SSD1         None        0           NaN   \n",
       "3                      CT250MX500SSD1         None        0           NaN   \n",
       "4                      CT250MX500SSD1         None        0           NaN   \n",
       "5                HGST HUH728080ALE604         HGST        0         15.45   \n",
       "6                HGST HUH728080ALE604         HGST        0         15.45   \n",
       "7                HGST HUH728080ALE604         HGST        0         15.45   \n",
       "8  Seagate BarraCuda SSD ZA250CM10002         None        0           NaN   \n",
       "9                       ST500LM012 HN      SEAGATE        0         25.12   \n",
       "\n",
       "   pct_two_star  total_reviews  capacity_bytes  \n",
       "0           NaN            NaN    250059350016  \n",
       "1           NaN            NaN    250059350016  \n",
       "2           NaN            NaN    250059350016  \n",
       "3           NaN            NaN    250059350016  \n",
       "4           NaN            NaN    250059350016  \n",
       "5          5.46         3263.0   8001563222016  \n",
       "6          5.46         3263.0   8001563222016  \n",
       "7          5.46         3263.0   8001563222016  \n",
       "8           NaN            NaN    250059350016  \n",
       "9          7.27        40251.0    500107862016  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Review feature statistics by manufacturer:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pct_one_star</th>\n",
       "      <th>pct_two_star</th>\n",
       "      <th>total_reviews</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturer</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>HGST</th>\n",
       "      <td>15.45</td>\n",
       "      <td>5.46</td>\n",
       "      <td>3263.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SEAGATE</th>\n",
       "      <td>25.12</td>\n",
       "      <td>7.27</td>\n",
       "      <td>40251.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TOSHIBA</th>\n",
       "      <td>17.01</td>\n",
       "      <td>6.10</td>\n",
       "      <td>16134.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WDC</th>\n",
       "      <td>13.55</td>\n",
       "      <td>5.77</td>\n",
       "      <td>103758.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              pct_one_star  pct_two_star  total_reviews\n",
       "manufacturer                                           \n",
       "HGST                 15.45          5.46         3263.0\n",
       "SEAGATE              25.12          7.27        40251.0\n",
       "TOSHIBA              17.01          6.10        16134.0\n",
       "WDC                  13.55          5.77       103758.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Checking for missing review features (manufacturers without reviews):\n",
      "Records without review features: 16,281 / 1,645,930\n",
      "\n",
      "Manufacturers with missing review data:\n",
      "Series([], Name: count, dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "# Fix manufacturer column in df_pandas to match standardized names\n",
    "# Reapply the get_bb_manufacturer function to ensure consistency\n",
    "\n",
    "def get_bb_manufacturer(model):\n",
    "    \"\"\"Extract manufacturer from Backblaze model name\"\"\"\n",
    "    model_str = str(model)\n",
    "    if model_str.startswith('TOSHIBA'):\n",
    "        return 'TOSHIBA'\n",
    "    elif model_str.startswith('ST') or model_str.startswith('SEAGATE'):\n",
    "        return 'SEAGATE'\n",
    "    elif model_str.startswith('WDC') or model_str.startswith('WD'):\n",
    "        return 'WDC'\n",
    "    elif model_str.startswith('HGST') or model_str.startswith('HITACHI'):\n",
    "        return 'HGST'\n",
    "    return None\n",
    "\n",
    "# Re-create manufacturer column with standardized names\n",
    "df_pandas['manufacturer'] = df_pandas['model'].apply(get_bb_manufacturer)\n",
    "\n",
    "print(\"Standardized manufacturer distribution in df_pandas:\")\n",
    "print(df_pandas['manufacturer'].value_counts())\n",
    "\n",
    "# Join the manufacturer rating features with the Backblaze data\n",
    "mfr_ratings_for_join = mfr_ratings[['manufacturer', 'pct_one_star', 'pct_two_star', 'total_reviews', \n",
    "                                     'one_star_count', 'two_star_count']].copy()\n",
    "\n",
    "# Perform the join on manufacturer column\n",
    "df_joined = df_pandas.merge(\n",
    "    mfr_ratings_for_join, \n",
    "    on='manufacturer', \n",
    "    how='left',\n",
    "    suffixes=('', '_reviews')\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"JOINED DATAFRAME: BACKBLAZE DATA + REVIEW FEATURES\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nOriginal df_pandas shape: {df_pandas.shape}\")\n",
    "print(f\"Joined df_joined shape: {df_joined.shape}\")\n",
    "print(f\"New columns added: {df_joined.shape[1] - df_pandas.shape[1]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"New review-based features added to each Backblaze record:\")\n",
    "print(\"  • total_reviews - Total number of reviews for this manufacturer\")\n",
    "print(\"  • one_star_count - Count of 1-star reviews for this manufacturer\")\n",
    "print(\"  • two_star_count - Count of 2-star reviews for this manufacturer\")\n",
    "print(\"  • pct_one_star - Percentage of 1-star reviews (quality indicator)\")\n",
    "print(\"  • pct_two_star - Percentage of 2-star reviews (quality indicator)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Sample rows with key columns:\")\n",
    "sample_cols = ['model', 'manufacturer', 'failure', 'pct_one_star', 'pct_two_star', \n",
    "               'total_reviews', 'capacity_bytes']\n",
    "# Only show columns that exist\n",
    "sample_cols_exist = [col for col in sample_cols if col in df_joined.columns]\n",
    "display(df_joined[sample_cols_exist].head(10))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Review feature statistics by manufacturer:\")\n",
    "display(df_joined.groupby('manufacturer')[['pct_one_star', 'pct_two_star', 'total_reviews']].first())\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Checking for missing review features (manufacturers without reviews):\")\n",
    "missing_reviews = df_joined['pct_one_star'].isna().sum()\n",
    "print(f\"Records without review features: {missing_reviews:,} / {len(df_joined):,}\")\n",
    "if missing_reviews > 0:\n",
    "    print(\"\\nManufacturers with missing review data:\")\n",
    "    print(df_joined[df_joined['pct_one_star'].isna()]['manufacturer'].value_counts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
