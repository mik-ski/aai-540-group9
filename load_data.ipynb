{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d4c6328",
   "metadata": {},
   "source": [
    "# Backblaze Data Pipeline - S3 Setup and Data Loading\n",
    "\n",
    "This notebook sets up an S3 bucket, downloads Backblaze hard drive data, and converts it to partitioned Parquet format for efficient querying.\n",
    "\n",
    "## 1. Initialize S3 Bucket\n",
    "\n",
    "Creates or retrieves an S3 bucket for storing raw and curated data. The bucket name is persisted in a `.env` file for reuse across sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43983a8e-a5ca-49f5-9ee6-57d4dfc8e9d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing bucket from .env: mlops-backblaze-d7b30cb5-us-east-1\n",
      "Bucket ready: mlops-backblaze-d7b30cb5-us-east-1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import uuid\n",
    "import boto3\n",
    "from dotenv import load_dotenv, set_key\n",
    "\n",
    "ENV_PATH = \".env\"\n",
    "\n",
    "# Load .env if it exists\n",
    "load_dotenv(ENV_PATH)\n",
    "\n",
    "bucket = os.getenv(\"BUCKET_NAME\")\n",
    "\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name\n",
    "s3 = boto3.client(\"s3\", region_name=region)\n",
    "\n",
    "# If BUCKET_NAME exists, use it\n",
    "if bucket:\n",
    "    print(\"Using existing bucket from .env:\", bucket)\n",
    "\n",
    "else:\n",
    "    # Create new bucket name\n",
    "    bucket = f\"mlops-backblaze-{uuid.uuid4().hex[:8]}-{region}\"\n",
    "    print(\"Creating new bucket:\", bucket)\n",
    "\n",
    "    if region == \"us-east-1\":\n",
    "        s3.create_bucket(Bucket=bucket)\n",
    "    else:\n",
    "        s3.create_bucket(\n",
    "            Bucket=bucket,\n",
    "            CreateBucketConfiguration={\"LocationConstraint\": region}\n",
    "        )\n",
    "\n",
    "    # Persist to .env\n",
    "    if not os.path.exists(ENV_PATH):\n",
    "        open(ENV_PATH, \"w\").close()\n",
    "\n",
    "    set_key(ENV_PATH, \"BUCKET_NAME\", bucket)\n",
    "\n",
    "print(\"Bucket ready:\", bucket)\n",
    "reload_s3 = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9832ee59",
   "metadata": {},
   "source": [
    "## 2. Create Folder Structure\n",
    "\n",
    "Creates a standard MLOps folder structure in S3 to organize:\n",
    "- Raw data (Backblaze hard drive stats, reviews)\n",
    "- Curated/processed data\n",
    "- Feature stores\n",
    "- Model artifacts\n",
    "- Evaluation results\n",
    "- Monitoring data\n",
    "- Batch inference outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f1d4fa7e-bd95-46ee-bcaa-a3344a97f221",
   "metadata": {},
   "outputs": [],
   "source": [
    "if reload_s3:\n",
    "    prefixes = [\n",
    "        \"raw/backblaze/\",\n",
    "        \"raw/reviews/\",\n",
    "        \"curated/\",\n",
    "        \"features/\",\n",
    "        \"artifacts/models/\",\n",
    "        \"artifacts/eval/\",\n",
    "        \"artifacts/monitoring/\",\n",
    "        \"inference/batch/\"\n",
    "    ]\n",
    "\n",
    "    for p in prefixes:\n",
    "        s3.put_object(Bucket=bucket, Key=p)\n",
    "\n",
    "    print(\"Folder layout created:\")\n",
    "    for p in prefixes:\n",
    "        print(\" -\", p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21545a3a",
   "metadata": {},
   "source": [
    "## 3. Verify Folder Structure\n",
    "\n",
    "Lists all objects in the S3 bucket to confirm the folder structure was created successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bd5e3eae-8202-4219-a984-5101e9bf9842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "artifacts/eval/\n",
      "artifacts/models/\n",
      "artifacts/monitoring/\n",
      "athena-results/232a73ba-0572-4020-aed2-dcf74ffb621a.csv\n",
      "athena-results/232a73ba-0572-4020-aed2-dcf74ffb621a.csv.metadata\n",
      "athena-results/79896f08-b110-4191-b48d-be402d728607.txt\n",
      "athena-results/8a0124c9-9d77-45bd-ac96-54bc7978095b.txt\n",
      "athena-results/dfe1c2df-e419-4136-9090-5756de766b40.txt\n",
      "athena-results/dfe1c2df-e419-4136-9090-5756de766b40.txt.metadata\n",
      "athena-results/ebefd43b-a818-42bf-8165-685a186dbbe4.txt\n",
      "athena-results/ebefd43b-a818-42bf-8165-685a186dbbe4.txt.metadata\n",
      "curated/\n",
      "curated/backblaze_parquet/year=2025/month=01/day=01/data_Q1_2025_01.parquet\n",
      "curated/backblaze_parquet/year=2025/month=01/day=02/data_Q1_2025_02.parquet\n",
      "curated/backblaze_parquet/year=2025/month=01/day=03/data_Q1_2025_03.parquet\n",
      "curated/backblaze_parquet/year=2025/month=01/day=04/data_Q1_2025_04.parquet\n",
      "curated/backblaze_parquet/year=2025/month=01/day=05/data_Q1_2025_05.parquet\n",
      "curated/backblaze_parquet/year=2025/month=01/day=06/data_Q1_2025_06.parquet\n",
      "curated/backblaze_parquet/year=2025/month=01/day=07/data_Q1_2025_07.parquet\n",
      "curated/backblaze_parquet/year=2025/month=01/day=08/data_Q1_2025_08.parquet\n",
      "curated/backblaze_parquet/year=2025/month=01/day=09/data_Q1_2025_09.parquet\n",
      "curated/backblaze_parquet/year=2025/month=01/day=10/data_Q1_2025_10.parquet\n",
      "curated/backblaze_parquet/year=2025/month=01/day=11/data_Q1_2025_11.parquet\n",
      "curated/backblaze_parquet/year=2025/month=01/day=12/data_Q1_2025_12.parquet\n",
      "curated/backblaze_parquet/year=2025/month=01/day=13/data_Q1_2025_13.parquet\n",
      "curated/backblaze_parquet/year=2025/month=01/day=14/data_Q1_2025_14.parquet\n",
      "curated/backblaze_parquet/year=2025/month=01/day=15/data_Q1_2025_15.parquet\n",
      "curated/backblaze_parquet/year=2025/month=01/day=16/data_Q1_2025_16.parquet\n",
      "curated/backblaze_parquet/year=2025/month=01/day=17/data_Q1_2025_17.parquet\n",
      "curated/backblaze_parquet/year=2025/month=01/day=18/data_Q1_2025_18.parquet\n",
      "curated/backblaze_parquet/year=2025/month=01/day=19/data_Q1_2025_19.parquet\n",
      "curated/backblaze_parquet/year=2025/month=01/day=20/data_Q1_2025_20.parquet\n",
      "curated/backblaze_parquet/year=2025/month=01/day=21/data_Q1_2025_21.parquet\n",
      "curated/backblaze_parquet/year=2025/month=01/day=22/data_Q1_2025_22.parquet\n",
      "curated/backblaze_parquet/year=2025/month=01/day=23/data_Q1_2025_23.parquet\n",
      "curated/backblaze_parquet/year=2025/month=01/day=24/data_Q1_2025_24.parquet\n",
      "curated/backblaze_parquet/year=2025/month=01/day=25/data_Q1_2025_25.parquet\n",
      "curated/backblaze_parquet/year=2025/month=01/day=26/data_Q1_2025_26.parquet\n",
      "curated/backblaze_parquet/year=2025/month=01/day=27/data_Q1_2025_27.parquet\n",
      "curated/backblaze_parquet/year=2025/month=01/day=28/data_Q1_2025_28.parquet\n",
      "curated/backblaze_parquet/year=2025/month=01/day=29/data_Q1_2025_29.parquet\n",
      "curated/backblaze_parquet/year=2025/month=01/day=30/data_Q1_2025_30.parquet\n",
      "curated/backblaze_parquet/year=2025/month=01/day=31/data_Q1_2025_31.parquet\n",
      "curated/backblaze_parquet/year=2025/month=02/day=01/data_Q1_2025_01.parquet\n",
      "curated/backblaze_parquet/year=2025/month=02/day=02/data_Q1_2025_02.parquet\n",
      "curated/backblaze_parquet/year=2025/month=02/day=03/data_Q1_2025_03.parquet\n",
      "curated/backblaze_parquet/year=2025/month=02/day=04/data_Q1_2025_04.parquet\n",
      "curated/backblaze_parquet/year=2025/month=02/day=05/data_Q1_2025_05.parquet\n",
      "curated/backblaze_parquet/year=2025/month=02/day=06/data_Q1_2025_06.parquet\n",
      "curated/backblaze_parquet/year=2025/month=02/day=07/data_Q1_2025_07.parquet\n",
      "curated/backblaze_parquet/year=2025/month=02/day=08/data_Q1_2025_08.parquet\n",
      "curated/backblaze_parquet/year=2025/month=02/day=09/data_Q1_2025_09.parquet\n",
      "curated/backblaze_parquet/year=2025/month=02/day=10/data_Q1_2025_10.parquet\n",
      "curated/backblaze_parquet/year=2025/month=02/day=11/data_Q1_2025_11.parquet\n",
      "curated/backblaze_parquet/year=2025/month=02/day=12/data_Q1_2025_12.parquet\n",
      "curated/backblaze_parquet/year=2025/month=02/day=13/data_Q1_2025_13.parquet\n",
      "curated/backblaze_parquet/year=2025/month=02/day=14/data_Q1_2025_14.parquet\n",
      "curated/backblaze_parquet/year=2025/month=02/day=15/data_Q1_2025_15.parquet\n",
      "curated/backblaze_parquet/year=2025/month=02/day=16/data_Q1_2025_16.parquet\n",
      "curated/backblaze_parquet/year=2025/month=02/day=17/data_Q1_2025_17.parquet\n",
      "curated/backblaze_parquet/year=2025/month=02/day=18/data_Q1_2025_18.parquet\n",
      "curated/backblaze_parquet/year=2025/month=02/day=19/data_Q1_2025_19.parquet\n",
      "curated/backblaze_parquet/year=2025/month=02/day=20/data_Q1_2025_20.parquet\n",
      "curated/backblaze_parquet/year=2025/month=02/day=21/data_Q1_2025_21.parquet\n",
      "curated/backblaze_parquet/year=2025/month=02/day=22/data_Q1_2025_22.parquet\n",
      "curated/backblaze_parquet/year=2025/month=02/day=23/data_Q1_2025_23.parquet\n",
      "curated/backblaze_parquet/year=2025/month=02/day=24/data_Q1_2025_24.parquet\n",
      "curated/backblaze_parquet/year=2025/month=02/day=25/data_Q1_2025_25.parquet\n",
      "curated/backblaze_parquet/year=2025/month=02/day=26/data_Q1_2025_26.parquet\n",
      "curated/backblaze_parquet/year=2025/month=02/day=27/data_Q1_2025_27.parquet\n",
      "curated/backblaze_parquet/year=2025/month=02/day=28/data_Q1_2025_28.parquet\n",
      "curated/backblaze_parquet/year=2025/month=03/day=01/data_Q1_2025_01.parquet\n",
      "curated/backblaze_parquet/year=2025/month=03/day=02/data_Q1_2025_02.parquet\n",
      "curated/backblaze_parquet/year=2025/month=03/day=03/data_Q1_2025_03.parquet\n",
      "curated/backblaze_parquet/year=2025/month=03/day=04/data_Q1_2025_04.parquet\n",
      "curated/backblaze_parquet/year=2025/month=03/day=05/data_Q1_2025_05.parquet\n",
      "curated/backblaze_parquet/year=2025/month=03/day=06/data_Q1_2025_06.parquet\n",
      "curated/backblaze_parquet/year=2025/month=03/day=07/data_Q1_2025_07.parquet\n",
      "curated/backblaze_parquet/year=2025/month=03/day=08/data_Q1_2025_08.parquet\n",
      "curated/backblaze_parquet/year=2025/month=03/day=09/data_Q1_2025_09.parquet\n",
      "curated/backblaze_parquet/year=2025/month=03/day=10/data_Q1_2025_10.parquet\n",
      "curated/backblaze_parquet/year=2025/month=03/day=11/data_Q1_2025_11.parquet\n",
      "curated/backblaze_parquet/year=2025/month=03/day=12/data_Q1_2025_12.parquet\n",
      "curated/backblaze_parquet/year=2025/month=03/day=13/data_Q1_2025_13.parquet\n",
      "curated/backblaze_parquet/year=2025/month=03/day=14/data_Q1_2025_14.parquet\n",
      "curated/backblaze_parquet/year=2025/month=03/day=15/data_Q1_2025_15.parquet\n",
      "curated/backblaze_parquet/year=2025/month=03/day=16/data_Q1_2025_16.parquet\n",
      "curated/backblaze_parquet/year=2025/month=03/day=17/data_Q1_2025_17.parquet\n",
      "curated/backblaze_parquet/year=2025/month=03/day=18/data_Q1_2025_18.parquet\n",
      "curated/backblaze_parquet/year=2025/month=03/day=19/data_Q1_2025_19.parquet\n",
      "curated/backblaze_parquet/year=2025/month=03/day=20/data_Q1_2025_20.parquet\n",
      "curated/backblaze_parquet/year=2025/month=03/day=21/data_Q1_2025_21.parquet\n",
      "curated/backblaze_parquet/year=2025/month=03/day=22/data_Q1_2025_22.parquet\n",
      "curated/backblaze_parquet/year=2025/month=03/day=23/data_Q1_2025_23.parquet\n",
      "curated/backblaze_parquet/year=2025/month=03/day=24/data_Q1_2025_24.parquet\n",
      "curated/backblaze_parquet/year=2025/month=03/day=25/data_Q1_2025_25.parquet\n",
      "curated/backblaze_parquet/year=2025/month=03/day=26/data_Q1_2025_26.parquet\n",
      "curated/backblaze_parquet/year=2025/month=03/day=27/data_Q1_2025_27.parquet\n",
      "curated/backblaze_parquet/year=2025/month=03/day=28/data_Q1_2025_28.parquet\n",
      "curated/backblaze_parquet/year=2025/month=03/day=29/data_Q1_2025_29.parquet\n",
      "curated/backblaze_parquet/year=2025/month=03/day=30/data_Q1_2025_30.parquet\n",
      "curated/backblaze_parquet/year=2025/month=03/day=31/data_Q1_2025_31.parquet\n",
      "curated/backblaze_parquet/year=2025/month=04/day=01/data_Q2_2025_01.parquet\n",
      "curated/backblaze_parquet/year=2025/month=04/day=02/data_Q2_2025_02.parquet\n",
      "curated/backblaze_parquet/year=2025/month=04/day=03/data_Q2_2025_03.parquet\n",
      "curated/backblaze_parquet/year=2025/month=04/day=04/data_Q2_2025_04.parquet\n",
      "curated/backblaze_parquet/year=2025/month=04/day=05/data_Q2_2025_05.parquet\n",
      "curated/backblaze_parquet/year=2025/month=04/day=06/data_Q2_2025_06.parquet\n",
      "curated/backblaze_parquet/year=2025/month=04/day=07/data_Q2_2025_07.parquet\n",
      "curated/backblaze_parquet/year=2025/month=04/day=08/data_Q2_2025_08.parquet\n",
      "curated/backblaze_parquet/year=2025/month=04/day=09/data_Q2_2025_09.parquet\n",
      "curated/backblaze_parquet/year=2025/month=04/day=10/data_Q2_2025_10.parquet\n",
      "curated/backblaze_parquet/year=2025/month=04/day=11/data_Q2_2025_11.parquet\n",
      "curated/backblaze_parquet/year=2025/month=04/day=12/data_Q2_2025_12.parquet\n",
      "curated/backblaze_parquet/year=2025/month=04/day=13/data_Q2_2025_13.parquet\n",
      "curated/backblaze_parquet/year=2025/month=04/day=14/data_Q2_2025_14.parquet\n",
      "curated/backblaze_parquet/year=2025/month=04/day=15/data_Q2_2025_15.parquet\n",
      "curated/backblaze_parquet/year=2025/month=04/day=16/data_Q2_2025_16.parquet\n",
      "curated/backblaze_parquet/year=2025/month=04/day=17/data_Q2_2025_17.parquet\n",
      "curated/backblaze_parquet/year=2025/month=04/day=18/data_Q2_2025_18.parquet\n",
      "curated/backblaze_parquet/year=2025/month=04/day=19/data_Q2_2025_19.parquet\n",
      "curated/backblaze_parquet/year=2025/month=04/day=20/data_Q2_2025_20.parquet\n",
      "curated/backblaze_parquet/year=2025/month=04/day=21/data_Q2_2025_21.parquet\n",
      "curated/backblaze_parquet/year=2025/month=04/day=22/data_Q2_2025_22.parquet\n",
      "curated/backblaze_parquet/year=2025/month=04/day=23/data_Q2_2025_23.parquet\n",
      "curated/backblaze_parquet/year=2025/month=04/day=24/data_Q2_2025_24.parquet\n",
      "curated/backblaze_parquet/year=2025/month=04/day=25/data_Q2_2025_25.parquet\n",
      "curated/backblaze_parquet/year=2025/month=04/day=26/data_Q2_2025_26.parquet\n",
      "curated/backblaze_parquet/year=2025/month=04/day=27/data_Q2_2025_27.parquet\n",
      "curated/backblaze_parquet/year=2025/month=04/day=28/data_Q2_2025_28.parquet\n",
      "curated/backblaze_parquet/year=2025/month=04/day=29/data_Q2_2025_29.parquet\n",
      "curated/backblaze_parquet/year=2025/month=04/day=30/data_Q2_2025_30.parquet\n",
      "curated/backblaze_parquet/year=2025/month=05/day=01/data_Q2_2025_01.parquet\n",
      "curated/backblaze_parquet/year=2025/month=05/day=02/data_Q2_2025_02.parquet\n",
      "curated/backblaze_parquet/year=2025/month=05/day=03/data_Q2_2025_03.parquet\n",
      "curated/backblaze_parquet/year=2025/month=05/day=04/data_Q2_2025_04.parquet\n",
      "curated/backblaze_parquet/year=2025/month=05/day=05/data_Q2_2025_05.parquet\n",
      "curated/backblaze_parquet/year=2025/month=05/day=06/data_Q2_2025_06.parquet\n",
      "curated/backblaze_parquet/year=2025/month=05/day=07/data_Q2_2025_07.parquet\n",
      "curated/backblaze_parquet/year=2025/month=05/day=08/data_Q2_2025_08.parquet\n",
      "curated/backblaze_parquet/year=2025/month=05/day=09/data_Q2_2025_09.parquet\n",
      "curated/backblaze_parquet/year=2025/month=05/day=10/data_Q2_2025_10.parquet\n",
      "curated/backblaze_parquet/year=2025/month=05/day=11/data_Q2_2025_11.parquet\n",
      "curated/backblaze_parquet/year=2025/month=05/day=12/data_Q2_2025_12.parquet\n",
      "curated/backblaze_parquet/year=2025/month=05/day=13/data_Q2_2025_13.parquet\n",
      "curated/backblaze_parquet/year=2025/month=05/day=14/data_Q2_2025_14.parquet\n",
      "curated/backblaze_parquet/year=2025/month=05/day=15/data_Q2_2025_15.parquet\n",
      "curated/backblaze_parquet/year=2025/month=05/day=16/data_Q2_2025_16.parquet\n",
      "curated/backblaze_parquet/year=2025/month=05/day=17/data_Q2_2025_17.parquet\n",
      "curated/backblaze_parquet/year=2025/month=05/day=18/data_Q2_2025_18.parquet\n",
      "curated/backblaze_parquet/year=2025/month=05/day=19/data_Q2_2025_19.parquet\n",
      "curated/backblaze_parquet/year=2025/month=05/day=20/data_Q2_2025_20.parquet\n",
      "curated/backblaze_parquet/year=2025/month=05/day=21/data_Q2_2025_21.parquet\n",
      "curated/backblaze_parquet/year=2025/month=05/day=22/data_Q2_2025_22.parquet\n",
      "curated/backblaze_parquet/year=2025/month=05/day=23/data_Q2_2025_23.parquet\n",
      "curated/backblaze_parquet/year=2025/month=05/day=24/data_Q2_2025_24.parquet\n",
      "curated/backblaze_parquet/year=2025/month=05/day=25/data_Q2_2025_25.parquet\n",
      "curated/backblaze_parquet/year=2025/month=05/day=26/data_Q2_2025_26.parquet\n",
      "curated/backblaze_parquet/year=2025/month=05/day=27/data_Q2_2025_27.parquet\n",
      "curated/backblaze_parquet/year=2025/month=05/day=28/data_Q2_2025_28.parquet\n",
      "curated/backblaze_parquet/year=2025/month=05/day=29/data_Q2_2025_29.parquet\n",
      "curated/backblaze_parquet/year=2025/month=05/day=30/data_Q2_2025_30.parquet\n",
      "curated/backblaze_parquet/year=2025/month=05/day=31/data_Q2_2025_31.parquet\n",
      "curated/backblaze_parquet/year=2025/month=06/day=01/data_Q2_2025_01.parquet\n",
      "curated/backblaze_parquet/year=2025/month=06/day=02/data_Q2_2025_02.parquet\n",
      "curated/backblaze_parquet/year=2025/month=06/day=03/data_Q2_2025_03.parquet\n",
      "curated/backblaze_parquet/year=2025/month=06/day=04/data_Q2_2025_04.parquet\n",
      "curated/backblaze_parquet/year=2025/month=06/day=05/data_Q2_2025_05.parquet\n",
      "curated/backblaze_parquet/year=2025/month=06/day=06/data_Q2_2025_06.parquet\n",
      "curated/backblaze_parquet/year=2025/month=06/day=07/data_Q2_2025_07.parquet\n",
      "curated/backblaze_parquet/year=2025/month=06/day=08/data_Q2_2025_08.parquet\n",
      "curated/backblaze_parquet/year=2025/month=06/day=09/data_Q2_2025_09.parquet\n",
      "curated/backblaze_parquet/year=2025/month=06/day=10/data_Q2_2025_10.parquet\n",
      "curated/backblaze_parquet/year=2025/month=06/day=11/data_Q2_2025_11.parquet\n",
      "curated/backblaze_parquet/year=2025/month=06/day=12/data_Q2_2025_12.parquet\n",
      "curated/backblaze_parquet/year=2025/month=06/day=13/data_Q2_2025_13.parquet\n",
      "curated/backblaze_parquet/year=2025/month=06/day=14/data_Q2_2025_14.parquet\n",
      "curated/backblaze_parquet/year=2025/month=06/day=15/data_Q2_2025_15.parquet\n",
      "curated/backblaze_parquet/year=2025/month=06/day=16/data_Q2_2025_16.parquet\n",
      "curated/backblaze_parquet/year=2025/month=06/day=17/data_Q2_2025_17.parquet\n",
      "curated/backblaze_parquet/year=2025/month=06/day=18/data_Q2_2025_18.parquet\n",
      "curated/backblaze_parquet/year=2025/month=06/day=19/data_Q2_2025_19.parquet\n",
      "curated/backblaze_parquet/year=2025/month=06/day=20/data_Q2_2025_20.parquet\n",
      "curated/backblaze_parquet/year=2025/month=06/day=21/data_Q2_2025_21.parquet\n",
      "curated/backblaze_parquet/year=2025/month=06/day=22/data_Q2_2025_22.parquet\n",
      "curated/backblaze_parquet/year=2025/month=06/day=23/data_Q2_2025_23.parquet\n",
      "curated/backblaze_parquet/year=2025/month=06/day=24/data_Q2_2025_24.parquet\n",
      "curated/backblaze_parquet/year=2025/month=06/day=25/data_Q2_2025_25.parquet\n",
      "curated/backblaze_parquet/year=2025/month=06/day=26/data_Q2_2025_26.parquet\n",
      "curated/backblaze_parquet/year=2025/month=06/day=27/data_Q2_2025_27.parquet\n",
      "curated/backblaze_parquet/year=2025/month=06/day=28/data_Q2_2025_28.parquet\n",
      "curated/backblaze_parquet/year=2025/month=06/day=29/data_Q2_2025_29.parquet\n",
      "curated/backblaze_parquet/year=2025/month=06/day=30/data_Q2_2025_30.parquet\n",
      "curated/backblaze_parquet/year=2025/month=07/day=01/data_Q3_2025_01.parquet\n",
      "curated/backblaze_parquet/year=2025/month=07/day=02/data_Q3_2025_02.parquet\n",
      "curated/backblaze_parquet/year=2025/month=07/day=03/data_Q3_2025_03.parquet\n",
      "curated/backblaze_parquet/year=2025/month=07/day=04/data_Q3_2025_04.parquet\n",
      "curated/backblaze_parquet/year=2025/month=07/day=05/data_Q3_2025_05.parquet\n",
      "curated/backblaze_parquet/year=2025/month=07/day=06/data_Q3_2025_06.parquet\n",
      "curated/backblaze_parquet/year=2025/month=07/day=07/data_Q3_2025_07.parquet\n",
      "curated/backblaze_parquet/year=2025/month=07/day=08/data_Q3_2025_08.parquet\n",
      "curated/backblaze_parquet/year=2025/month=07/day=09/data_Q3_2025_09.parquet\n",
      "curated/backblaze_parquet/year=2025/month=07/day=10/data_Q3_2025_10.parquet\n",
      "curated/backblaze_parquet/year=2025/month=07/day=11/data_Q3_2025_11.parquet\n",
      "curated/backblaze_parquet/year=2025/month=07/day=12/data_Q3_2025_12.parquet\n",
      "curated/backblaze_parquet/year=2025/month=07/day=13/data_Q3_2025_13.parquet\n",
      "curated/backblaze_parquet/year=2025/month=07/day=14/data_Q3_2025_14.parquet\n",
      "curated/backblaze_parquet/year=2025/month=07/day=15/data_Q3_2025_15.parquet\n",
      "curated/backblaze_parquet/year=2025/month=07/day=16/data_Q3_2025_16.parquet\n",
      "curated/backblaze_parquet/year=2025/month=07/day=17/data_Q3_2025_17.parquet\n",
      "curated/backblaze_parquet/year=2025/month=07/day=18/data_Q3_2025_18.parquet\n",
      "curated/backblaze_parquet/year=2025/month=07/day=19/data_Q3_2025_19.parquet\n",
      "curated/backblaze_parquet/year=2025/month=07/day=20/data_Q3_2025_20.parquet\n",
      "curated/backblaze_parquet/year=2025/month=07/day=21/data_Q3_2025_21.parquet\n",
      "curated/backblaze_parquet/year=2025/month=07/day=22/data_Q3_2025_22.parquet\n",
      "curated/backblaze_parquet/year=2025/month=07/day=23/data_Q3_2025_23.parquet\n",
      "curated/backblaze_parquet/year=2025/month=07/day=24/data_Q3_2025_24.parquet\n",
      "curated/backblaze_parquet/year=2025/month=07/day=25/data_Q3_2025_25.parquet\n",
      "curated/backblaze_parquet/year=2025/month=07/day=26/data_Q3_2025_26.parquet\n",
      "curated/backblaze_parquet/year=2025/month=07/day=27/data_Q3_2025_27.parquet\n",
      "curated/backblaze_parquet/year=2025/month=07/day=28/data_Q3_2025_28.parquet\n",
      "curated/backblaze_parquet/year=2025/month=07/day=29/data_Q3_2025_29.parquet\n",
      "curated/backblaze_parquet/year=2025/month=07/day=30/data_Q3_2025_30.parquet\n",
      "curated/backblaze_parquet/year=2025/month=07/day=31/data_Q3_2025_31.parquet\n",
      "curated/backblaze_parquet/year=2025/month=08/day=01/data_Q3_2025_01.parquet\n",
      "curated/backblaze_parquet/year=2025/month=08/day=02/data_Q3_2025_02.parquet\n",
      "curated/backblaze_parquet/year=2025/month=08/day=03/data_Q3_2025_03.parquet\n",
      "curated/backblaze_parquet/year=2025/month=08/day=04/data_Q3_2025_04.parquet\n",
      "curated/backblaze_parquet/year=2025/month=08/day=05/data_Q3_2025_05.parquet\n",
      "curated/backblaze_parquet/year=2025/month=08/day=06/data_Q3_2025_06.parquet\n",
      "curated/backblaze_parquet/year=2025/month=08/day=07/data_Q3_2025_07.parquet\n",
      "curated/backblaze_parquet/year=2025/month=08/day=08/data_Q3_2025_08.parquet\n",
      "curated/backblaze_parquet/year=2025/month=08/day=09/data_Q3_2025_09.parquet\n",
      "curated/backblaze_parquet/year=2025/month=08/day=10/data_Q3_2025_10.parquet\n",
      "curated/backblaze_parquet/year=2025/month=08/day=11/data_Q3_2025_11.parquet\n",
      "curated/backblaze_parquet/year=2025/month=08/day=12/data_Q3_2025_12.parquet\n",
      "curated/backblaze_parquet/year=2025/month=08/day=13/data_Q3_2025_13.parquet\n",
      "curated/backblaze_parquet/year=2025/month=08/day=14/data_Q3_2025_14.parquet\n",
      "curated/backblaze_parquet/year=2025/month=08/day=15/data_Q3_2025_15.parquet\n",
      "curated/backblaze_parquet/year=2025/month=08/day=16/data_Q3_2025_16.parquet\n",
      "curated/backblaze_parquet/year=2025/month=08/day=17/data_Q3_2025_17.parquet\n",
      "curated/backblaze_parquet/year=2025/month=08/day=18/data_Q3_2025_18.parquet\n",
      "curated/backblaze_parquet/year=2025/month=08/day=19/data_Q3_2025_19.parquet\n",
      "curated/backblaze_parquet/year=2025/month=08/day=20/data_Q3_2025_20.parquet\n",
      "curated/backblaze_parquet/year=2025/month=08/day=21/data_Q3_2025_21.parquet\n",
      "curated/backblaze_parquet/year=2025/month=08/day=22/data_Q3_2025_22.parquet\n",
      "curated/backblaze_parquet/year=2025/month=08/day=23/data_Q3_2025_23.parquet\n",
      "curated/backblaze_parquet/year=2025/month=08/day=24/data_Q3_2025_24.parquet\n",
      "curated/backblaze_parquet/year=2025/month=08/day=25/data_Q3_2025_25.parquet\n",
      "curated/backblaze_parquet/year=2025/month=08/day=26/data_Q3_2025_26.parquet\n",
      "curated/backblaze_parquet/year=2025/month=08/day=27/data_Q3_2025_27.parquet\n",
      "curated/backblaze_parquet/year=2025/month=08/day=28/data_Q3_2025_28.parquet\n",
      "curated/backblaze_parquet/year=2025/month=08/day=29/data_Q3_2025_29.parquet\n",
      "curated/backblaze_parquet/year=2025/month=08/day=30/data_Q3_2025_30.parquet\n",
      "curated/backblaze_parquet/year=2025/month=08/day=31/data_Q3_2025_31.parquet\n",
      "curated/backblaze_parquet/year=2025/month=09/day=01/data_Q3_2025_01.parquet\n",
      "curated/backblaze_parquet/year=2025/month=09/day=02/data_Q3_2025_02.parquet\n",
      "curated/backblaze_parquet/year=2025/month=09/day=03/data_Q3_2025_03.parquet\n",
      "curated/backblaze_parquet/year=2025/month=09/day=04/data_Q3_2025_04.parquet\n",
      "curated/backblaze_parquet/year=2025/month=09/day=05/data_Q3_2025_05.parquet\n",
      "curated/backblaze_parquet/year=2025/month=09/day=06/data_Q3_2025_06.parquet\n",
      "curated/backblaze_parquet/year=2025/month=09/day=07/data_Q3_2025_07.parquet\n",
      "curated/backblaze_parquet/year=2025/month=09/day=08/data_Q3_2025_08.parquet\n",
      "curated/backblaze_parquet/year=2025/month=09/day=09/data_Q3_2025_09.parquet\n",
      "curated/backblaze_parquet/year=2025/month=09/day=10/data_Q3_2025_10.parquet\n",
      "curated/backblaze_parquet/year=2025/month=09/day=11/data_Q3_2025_11.parquet\n",
      "curated/backblaze_parquet/year=2025/month=09/day=12/data_Q3_2025_12.parquet\n",
      "curated/backblaze_parquet/year=2025/month=09/day=13/data_Q3_2025_13.parquet\n",
      "curated/backblaze_parquet/year=2025/month=09/day=14/data_Q3_2025_14.parquet\n",
      "curated/backblaze_parquet/year=2025/month=09/day=15/data_Q3_2025_15.parquet\n",
      "curated/backblaze_parquet/year=2025/month=09/day=16/data_Q3_2025_16.parquet\n",
      "curated/backblaze_parquet/year=2025/month=09/day=17/data_Q3_2025_17.parquet\n",
      "curated/backblaze_parquet/year=2025/month=09/day=18/data_Q3_2025_18.parquet\n",
      "curated/backblaze_parquet/year=2025/month=09/day=19/data_Q3_2025_19.parquet\n",
      "curated/backblaze_parquet/year=2025/month=09/day=20/data_Q3_2025_20.parquet\n",
      "curated/backblaze_parquet/year=2025/month=09/day=21/data_Q3_2025_21.parquet\n",
      "curated/backblaze_parquet/year=2025/month=09/day=22/data_Q3_2025_22.parquet\n",
      "curated/backblaze_parquet/year=2025/month=09/day=23/data_Q3_2025_23.parquet\n",
      "curated/backblaze_parquet/year=2025/month=09/day=24/data_Q3_2025_24.parquet\n",
      "curated/backblaze_parquet/year=2025/month=09/day=25/data_Q3_2025_25.parquet\n",
      "curated/backblaze_parquet/year=2025/month=09/day=26/data_Q3_2025_26.parquet\n",
      "curated/backblaze_parquet/year=2025/month=09/day=27/data_Q3_2025_27.parquet\n",
      "curated/backblaze_parquet/year=2025/month=09/day=28/data_Q3_2025_28.parquet\n",
      "curated/backblaze_parquet/year=2025/month=09/day=29/data_Q3_2025_29.parquet\n",
      "curated/backblaze_parquet/year=2025/month=09/day=30/data_Q3_2025_30.parquet\n",
      "features/\n",
      "inference/batch/\n",
      "raw/backblaze/\n",
      "raw/backblaze/zips/data_Q1_2025.zip\n",
      "raw/backblaze/zips/data_Q2_2025.zip\n",
      "raw/backblaze/zips/data_Q3_2025.zip\n",
      "raw/reviews/\n",
      "raw/reviews_2023_parquet/raw_review_Electronics/part-000000.parquet\n"
     ]
    }
   ],
   "source": [
    "resp = s3.list_objects_v2(Bucket=bucket)\n",
    "for obj in resp.get(\"Contents\", []):\n",
    "    print(obj[\"Key\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d9f2c7",
   "metadata": {},
   "source": [
    "## 4. Download Backblaze Data to S3\n",
    "\n",
    "Scrapes the Backblaze website to find all quarterly data files from 2024 onwards, then:\n",
    "- Downloads each ZIP file directly from Backblaze\n",
    "- Streams the data to S3 without storing locally (efficient for large files)\n",
    "- Skips files that already exist in S3 to avoid redundant downloads\n",
    "- Stores ZIP files in `raw/backblaze/zips/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "23a20ee4-0351-4641-95f6-ef4301917a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found URLs:\n",
      "3 backblaze_2025_plus_urls.txt\n",
      "https://f001.backblazeb2.com/file/Backblaze-Hard-Drive-Data/data_Q1_2025.zip\n",
      "https://f001.backblazeb2.com/file/Backblaze-Hard-Drive-Data/data_Q2_2025.zip\n",
      "https://f001.backblazeb2.com/file/Backblaze-Hard-Drive-Data/data_Q3_2025.zip\n",
      "Skip: data_Q1_2025.zip\n",
      "Skip: data_Q2_2025.zip\n",
      "Skip: data_Q3_2025.zip\n",
      "Done. Uploaded files in:\n",
      "2026-01-25 13:42:10 1020483699 data_Q1_2025.zip\n",
      "2026-01-25 13:43:05 1067562257 data_Q2_2025.zip\n",
      "2026-01-25 13:44:03 1111587745 data_Q3_2025.zip\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "set -e\n",
    "\n",
    "BUCKET=\"mlops-backblaze-d7b30cb5-us-east-1\"\n",
    "\n",
    "curl -s https://www.backblaze.com/cloud-storage/resources/hard-drive-test-data \\\n",
    "  | grep -Eo 'https://f001\\.backblazeb2\\.com/file/Backblaze-Hard-Drive-Data/data_Q[1-4]_[0-9]{4}\\.zip' \\\n",
    "  | grep -E '_202[5-9]\\.zip' \\\n",
    "  | sort -u > backblaze_2025_plus_urls.txt\n",
    "\n",
    "echo \"Found URLs:\"\n",
    "wc -l backblaze_2025_plus_urls.txt\n",
    "head -n 5 backblaze_2025_plus_urls.txt\n",
    "\n",
    "while read -r url; do\n",
    "  fname=$(basename \"$url\")\n",
    "\n",
    "  # Skip if already in S3\n",
    "  if aws s3 ls \"s3://$BUCKET/raw/backblaze/zips/$fname\" >/dev/null 2>&1; then\n",
    "    echo \"Skip: $fname\"\n",
    "    continue\n",
    "  fi\n",
    "\n",
    "  echo \"Streaming upload: $fname\"\n",
    "  wget -qO- \"$url\" | aws s3 cp - \"s3://$BUCKET/raw/backblaze/zips/$fname\"\n",
    "done < backblaze_2025_plus_urls.txt\n",
    "\n",
    "echo \"Done. Uploaded files in:\"\n",
    "aws s3 ls \"s3://$BUCKET/raw/backblaze/zips/\" | head\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122f88a5",
   "metadata": {},
   "source": [
    "## 5. List Downloaded ZIP Files\n",
    "\n",
    "Verifies that all ZIP files were successfully uploaded to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "246621ee-6c6b-449a-a3c2-41800c04fa68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-25 13:42:10 1020483699 data_Q1_2025.zip\n",
      "2026-01-25 13:43:05 1067562257 data_Q2_2025.zip\n",
      "2026-01-25 13:44:03 1111587745 data_Q3_2025.zip\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "BUCKET=\"mlops-backblaze-d7b30cb5-us-east-1\"\n",
    "aws s3 ls \"s3://$BUCKET/raw/backblaze/zips/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b966074",
   "metadata": {},
   "source": [
    "## 6. List and Prepare ZIP Files for Processing\n",
    "\n",
    "Imports libraries for data processing and lists all ZIP files from S3 that need to be converted to Parquet format. Uses pagination to handle large numbers of files efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "33ffcf69-ed43-439c-bae0-e1ad343374ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZIPs found: 3\n",
      "['raw/backblaze/zips/data_Q1_2025.zip', 'raw/backblaze/zips/data_Q2_2025.zip', 'raw/backblaze/zips/data_Q3_2025.zip']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from io import BytesIO\n",
    "import zipfile\n",
    "\n",
    "zip_s3_prefix = \"raw/backblaze/zips/\"\n",
    "out_prefix = \"curated/backblaze_parquet/\"\n",
    "\n",
    "# s3 = boto3.client(\"s3\")\n",
    "\n",
    "def list_s3_keys(prefix):\n",
    "    paginator = s3.get_paginator(\"list_objects_v2\")\n",
    "    keys = []\n",
    "    for page in paginator.paginate(Bucket=bucket, Prefix=prefix):\n",
    "        for obj in page.get(\"Contents\", []):\n",
    "            if obj[\"Key\"].endswith(\".zip\"):\n",
    "                keys.append(obj[\"Key\"])\n",
    "    return sorted(keys)\n",
    "\n",
    "zip_keys = list_s3_keys(zip_s3_prefix)\n",
    "print(\"ZIPs found:\", len(zip_keys))\n",
    "print(zip_keys[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522fbf59",
   "metadata": {},
   "source": [
    "## 7. Convert CSV to Partitioned Parquet\n",
    "\n",
    "Processes each ZIP file by:\n",
    "1. Downloading the ZIP from S3 into memory\n",
    "2. Extracting CSV files from the ZIP\n",
    "3. Parsing date information from CSV filenames (YYYY-MM-DD)\n",
    "4. Converting CSV to Parquet format with Snappy compression\n",
    "5. Uploading to S3 with Hive-style partitioning: `year=YYYY/month=MM/day=DD/`\n",
    "6. Skipping files that already exist to enable resumable processing\n",
    "\n",
    "This partitioned structure enables efficient querying by date ranges in tools like Athena or Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "289feb89-baac-45f0-be3c-4a88961a24d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if reload_s3:\n",
    "    def upload_parquet_to_s3(df, s3_key):\n",
    "        table = pa.Table.from_pandas(df, preserve_index=False)\n",
    "        buf = BytesIO()\n",
    "        pq.write_table(table, buf, compression=\"snappy\")\n",
    "        buf.seek(0)\n",
    "        s3.put_object(Bucket=bucket, Key=s3_key, Body=buf.getvalue())\n",
    "\n",
    "    def check_s3_key_exists(s3_key):\n",
    "        try:\n",
    "            s3.head_object(Bucket=bucket, Key=s3_key)\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "    for zip_key in zip_keys:\n",
    "        print(\"Processing:\", zip_key)\n",
    "        zip_obj = s3.get_object(Bucket=bucket, Key=zip_key)[\"Body\"].read()\n",
    "\n",
    "        with zipfile.ZipFile(BytesIO(zip_obj), \"r\") as z:\n",
    "            # Filter out macOS metadata files (._filename) and only get actual CSV files\n",
    "            csv_files = [n for n in z.namelist() \n",
    "                        if n.endswith(\".csv\") and not os.path.basename(n).startswith(\"._\")]\n",
    "\n",
    "            for csv_name in csv_files:\n",
    "                # csv_name is like 'data_Q1_2024/2024-01-01.csv'\n",
    "                date_str = os.path.basename(csv_name).replace(\".csv\", \"\")\n",
    "                yyyy, mm, dd = date_str.split(\"-\")\n",
    "\n",
    "                # Write to partitioned parquet key\n",
    "                out_key = (\n",
    "                    f\"{out_prefix}year={yyyy}/month={mm}/day={dd}/\"\n",
    "                    f\"{os.path.basename(zip_key).replace('.zip','')}_{dd}.parquet\"\n",
    "                )\n",
    "\n",
    "                # Check if already extracted\n",
    "                if check_s3_key_exists(out_key):\n",
    "                    print(f\"  Skip (already exists): {out_key}\")\n",
    "                    continue\n",
    "\n",
    "                with z.open(csv_name) as f:\n",
    "                    df = pd.read_csv(f, encoding='utf-8', encoding_errors='replace')\n",
    "\n",
    "                upload_parquet_to_s3(df, out_key)\n",
    "                print(f\"  Uploaded: {out_key}\")\n",
    "\n",
    "    print(\"Done writing curated partitioned parquet to S3!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b16468ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -q -U datasets pyarrow s3fs pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7465a499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://mlops-backblaze-d7b30cb5-us-east-1/raw/reviews_2023_parquet/raw_review_Electronics/\n",
      "s3://mlops-backblaze-d7b30cb5-us-east-1/raw/reviews_2023_parquet/raw_meta_Electronics/\n"
     ]
    }
   ],
   "source": [
    "HF_DATASET = \"McAuley-Lab/Amazon-Reviews-2023\"\n",
    "\n",
    "REVIEW_CONFIG = \"raw_review_Electronics\"\n",
    "META_CONFIG   = \"raw_meta_Electronics\"\n",
    "\n",
    "s3_reviews_out = f\"s3://{bucket}/raw/reviews_2023_parquet/{REVIEW_CONFIG}/\"\n",
    "s3_meta_out    = f\"s3://{bucket}/raw/reviews_2023_parquet/{META_CONFIG}/\"\n",
    "\n",
    "print(s3_reviews_out)\n",
    "print(s3_meta_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "50e8eecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if reload_s3:\n",
    "    from huggingface_hub import HfFileSystem\n",
    "    from pathlib import Path\n",
    "    import pandas as pd\n",
    "    import pyarrow.parquet as pq\n",
    "    import pyarrow as pa\n",
    "    from io import BytesIO\n",
    "    import json\n",
    "\n",
    "    # Initialize HF filesystem\n",
    "    hf_fs = HfFileSystem()\n",
    "\n",
    "    dataset_name = HF_DATASET\n",
    "\n",
    "    # Find Electronics review file\n",
    "    print(\"Finding Electronics review file...\")\n",
    "    review_path = f\"datasets/{dataset_name}/raw/review_categories\"\n",
    "    review_files = hf_fs.ls(review_path, detail=True)\n",
    "    electronics_files = [f for f in review_files if 'Electronics' in f['name']]\n",
    "\n",
    "    if not electronics_files:\n",
    "        print(\"No Electronics files found\")\n",
    "    else:\n",
    "        file_info = electronics_files[0]\n",
    "        file_path = file_info['name']\n",
    "        file_name = Path(file_path).name\n",
    "        \n",
    "        print(f\"Found: {file_name}\")\n",
    "        print(f\"Size: {file_info['size'] / 1024 / 1024:.2f} MB\")\n",
    "        \n",
    "        # First pass: Count total rows in JSONL file\n",
    "        print(f\"\\nCounting total rows in JSONL file...\")\n",
    "        total_lines = 0\n",
    "        with hf_fs.open(file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                if line.strip():\n",
    "                    total_lines += 1\n",
    "                if total_lines % 500000 == 0:\n",
    "                    print(f\"  Counted {total_lines:,} rows...\")\n",
    "        \n",
    "        print(f\"Total rows in JSONL file: {total_lines:,}\\n\")\n",
    "        \n",
    "        # Second pass: Stream JSONL from HF, convert to Parquet, upload directly to S3\n",
    "        # This avoids filling up local disk\n",
    "        # Filter for hard drive manufacturers: Western Digital, Toshiba, Seagate, Hitachi\n",
    "        print(f\"Streaming JSONL -> Parquet -> S3 (filtering for hard drive brands)...\")\n",
    "        \n",
    "        # Define target brands for filtering (case-insensitive)\n",
    "        TARGET_BRANDS = ['western digital', 'wd', 'toshiba', 'seagate', 'hitachi', 'hgst', 'wdc']\n",
    "        \n",
    "        def is_hard_drive_review(review_data):\n",
    "            \"\"\"Check if review is for Western Digital, Toshiba, Seagate, or Hitachi storage drives\"\"\"\n",
    "            # Check in title, text, and parent_asin fields\n",
    "            search_fields = []\n",
    "            \n",
    "            if 'title' in review_data and review_data['title']:\n",
    "                search_fields.append(str(review_data['title']).lower())\n",
    "            if 'text' in review_data and review_data['text']:\n",
    "                search_fields.append(str(review_data['text']).lower())\n",
    "            if 'parent_asin' in review_data:\n",
    "                search_fields.append(str(review_data['parent_asin']).lower())\n",
    "            \n",
    "            # Combine all searchable text\n",
    "            search_text = ' '.join(search_fields)\n",
    "            \n",
    "            # Check if any target brand appears in the text\n",
    "            for brand in TARGET_BRANDS:\n",
    "                if brand in search_text:\n",
    "                    # Additional check for storage-related keywords\n",
    "                    storage_keywords = ['hard drive', 'hdd', 'ssd', 'drive', 'storage', 'disk', 'external drive', 'internal drive']\n",
    "                    if any(keyword in search_text for keyword in storage_keywords):\n",
    "                        return True\n",
    "            return False\n",
    "        \n",
    "        chunk_size = 200_000\n",
    "        part_idx = 0\n",
    "        buffer = []\n",
    "        total_rows = 0\n",
    "        filtered_rows = 0\n",
    "        \n",
    "        # Open the JSONL file from HF\n",
    "        with hf_fs.open(file_path, 'r') as f:\n",
    "            for line_num, line in enumerate(f):\n",
    "                if line.strip():\n",
    "                    review_data = json.loads(line)\n",
    "                    total_rows += 1\n",
    "                    \n",
    "                    # Filter for hard drive brands\n",
    "                    if is_hard_drive_review(review_data):\n",
    "                        buffer.append(review_data)\n",
    "                        filtered_rows += 1\n",
    "                    \n",
    "                    # When buffer reaches chunk_size, write to parquet and upload\n",
    "                    if len(buffer) >= chunk_size:\n",
    "                        df = pd.DataFrame(buffer)\n",
    "                        table = pa.Table.from_pandas(df, preserve_index=False)\n",
    "                        \n",
    "                        # Write to memory buffer\n",
    "                        buf = BytesIO()\n",
    "                        pq.write_table(table, buf, compression=\"snappy\")\n",
    "                        buf.seek(0)\n",
    "                        \n",
    "                        # Upload directly to S3\n",
    "                        s3_key = s3_reviews_out.replace(f\"s3://{bucket}/\", \"\") + f\"part-{part_idx:06d}.parquet\"\n",
    "                        s3.put_object(Bucket=bucket, Key=s3_key, Body=buf.getvalue())\n",
    "                        \n",
    "                        print(f\"  Part {part_idx}: {len(buffer):,} rows → s3://{bucket}/{s3_key}\")\n",
    "                        \n",
    "                        # Clear buffer\n",
    "                        buffer = []\n",
    "                        part_idx += 1\n",
    "                \n",
    "                # Progress update every 100k lines\n",
    "                if (line_num + 1) % 100000 == 0:\n",
    "                    print(f\"  Processed {line_num + 1:,} lines | Filtered: {filtered_rows:,}/{total_rows:,} ({100*filtered_rows/total_rows:.2f}%)\")\n",
    "        \n",
    "        # Write remaining buffer\n",
    "        if buffer:\n",
    "            df = pd.DataFrame(buffer)\n",
    "            table = pa.Table.from_pandas(df, preserve_index=False)\n",
    "            \n",
    "            buf = BytesIO()\n",
    "            pq.write_table(table, buf, compression=\"snappy\")\n",
    "            buf.seek(0)\n",
    "            \n",
    "            s3_key = s3_reviews_out.replace(f\"s3://{bucket}/\", \"\") + f\"part-{part_idx:06d}.parquet\"\n",
    "            s3.put_object(Bucket=bucket, Key=s3_key, Body=buf.getvalue())\n",
    "            \n",
    "            print(f\"  Part {part_idx}: {len(buffer):,} rows → s3://{bucket}/{s3_key}\")\n",
    "        \n",
    "        print(f\"\\n Complete!\")\n",
    "        print(f\"   Total reviews processed: {total_rows:,}\")\n",
    "        print(f\"   Hard drive reviews (WD/Toshiba/Seagate/Hitachi): {filtered_rows:,} ({100*filtered_rows/total_rows:.2f}%)\")\n",
    "        print(f\"   Parquet files created: {part_idx + 1}\")\n",
    "        print(f\"   S3 location: {s3_reviews_out}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdaa84a",
   "metadata": {},
   "source": [
    "## Read Sample Backblaze Parquet File from S3\n",
    "\n",
    "Read a sample parquet file from the curated Backblaze data in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "58c1257d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if reload_s3:\n",
    "    import pandas as pd\n",
    "    import pyarrow.parquet as pq\n",
    "    import pyarrow as pa\n",
    "    from io import BytesIO\n",
    "\n",
    "    # List files in the S3 path\n",
    "    s3_path = \"curated/backblaze_parquet/\"\n",
    "    response = s3.list_objects_v2(Bucket=bucket, Prefix=s3_path, MaxKeys=10)\n",
    "\n",
    "    if 'Contents' in response:\n",
    "        # Get the first parquet file\n",
    "        parquet_files = [obj['Key'] for obj in response['Contents'] if obj['Key'].endswith('.parquet')]\n",
    "        \n",
    "        if parquet_files:\n",
    "            sample_file = parquet_files[0]\n",
    "            print(f\"Reading sample file: s3://{bucket}/{sample_file}\")\n",
    "            \n",
    "            # Download and read the parquet file\n",
    "            obj = s3.get_object(Bucket=bucket, Key=sample_file)\n",
    "            buffer = BytesIO(obj['Body'].read())\n",
    "            \n",
    "            # Read into dataframe\n",
    "            df_backblaze = pd.read_parquet(buffer)\n",
    "            \n",
    "            print(f\"\\nDataframe shape: {df_backblaze.shape}\")\n",
    "            print(f\"\\nColumns: {list(df_backblaze.columns)}\")\n",
    "            print(f\"\\nFirst few rows:\")\n",
    "            display(df_backblaze.head())\n",
    "        else:\n",
    "            print(f\"No parquet files found in s3://{bucket}/{s3_path}\")\n",
    "    else:\n",
    "        print(f\"No objects found in s3://{bucket}/{s3_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77588a13",
   "metadata": {},
   "source": [
    "## Read Sample Reviews Parquet File from S3\n",
    "\n",
    "Read a sample parquet file from the curated reviews data in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "86b4d1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "if reload_s3:\n",
    "    # List files in the reviews S3 path\n",
    "    s3_reviews_path = \"raw/reviews_2023_parquet/raw_review_Electronics/\"\n",
    "    response_reviews = s3.list_objects_v2(Bucket=bucket, Prefix=s3_reviews_path, MaxKeys=10)\n",
    "\n",
    "    if 'Contents' in response_reviews:\n",
    "        # Get the first parquet file\n",
    "        reviews_parquet_files = [obj['Key'] for obj in response_reviews['Contents'] if obj['Key'].endswith('.parquet')]\n",
    "        \n",
    "        if reviews_parquet_files:\n",
    "            sample_reviews_file = reviews_parquet_files[0]\n",
    "            print(f\"Reading sample reviews file: s3://{bucket}/{sample_reviews_file}\")\n",
    "            \n",
    "            # Download and read the parquet file\n",
    "            obj_reviews = s3.get_object(Bucket=bucket, Key=sample_reviews_file)\n",
    "            buffer_reviews = BytesIO(obj_reviews['Body'].read())\n",
    "            \n",
    "            # Read into dataframe\n",
    "            df_reviews = pd.read_parquet(buffer_reviews)\n",
    "            \n",
    "            print(f\"\\nDataframe shape: {df_reviews.shape}\")\n",
    "            print(f\"\\nColumns: {list(df_reviews.columns)}\")\n",
    "            print(f\"\\nFirst few rows:\")\n",
    "            display(df_reviews.head())\n",
    "        else:\n",
    "            print(f\"No parquet files found in s3://{bucket}/{s3_reviews_path}\")\n",
    "    else:\n",
    "        print(f\"No objects found in s3://{bucket}/{s3_reviews_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "63456818",
   "metadata": {},
   "outputs": [],
   "source": [
    "if reload_s3:\n",
    "    # Count unique ASIN values\n",
    "    unique_asin_count = df_reviews['asin'].nunique()\n",
    "    print(f\"Unique ASIN values: {unique_asin_count:,}\")\n",
    "\n",
    "    # Also show unique parent_asin if it exists\n",
    "    if 'parent_asin' in df_reviews.columns:\n",
    "        unique_parent_asin_count = df_reviews['parent_asin'].nunique()\n",
    "        print(f\"Unique parent_asin values: {unique_parent_asin_count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859957ba",
   "metadata": {},
   "source": [
    "## Examine Data for Joining\n",
    "\n",
    "Check sample values from both datasets to understand how to create a join key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "146e589c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if reload_s3:\n",
    "    # Sample review titles and parent_asin to see product names\n",
    "    print(\"Sample Reviews Data:\")\n",
    "    print(df_reviews[['parent_asin', 'title', 'text']].head(10))\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "    # Sample backblaze model names\n",
    "    print(\"Sample Backblaze Model Data:\")\n",
    "    print(df_backblaze['model'].value_counts().head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818a1c9a",
   "metadata": {},
   "source": [
    "## Create Join Keys\n",
    "\n",
    "Extract manufacturer and model information from reviews to match with Backblaze model data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "189bd518",
   "metadata": {},
   "outputs": [],
   "source": [
    "if reload_s3:\n",
    "    import re\n",
    "\n",
    "    def extract_manufacturer(text):\n",
    "        \"\"\"Extract manufacturer from review text\"\"\"\n",
    "        text_lower = str(text).lower()\n",
    "        \n",
    "        # Map various manufacturer names to standard format\n",
    "        if 'toshiba' in text_lower:\n",
    "            return 'TOSHIBA'\n",
    "        elif 'seagate' in text_lower or 'st' in text_lower[:3]:\n",
    "            return 'SEAGATE'\n",
    "        elif 'western digital' in text_lower or 'wd' in text_lower or 'wdc' in text_lower:\n",
    "            return 'WDC'\n",
    "        elif 'hitachi' in text_lower or 'hgst' in text_lower:\n",
    "            return 'HGST'\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def extract_model_hints(text):\n",
    "        \"\"\"Extract potential model numbers/patterns from review text\"\"\"\n",
    "        text_upper = str(text).upper()\n",
    "        \n",
    "        # Look for common model patterns\n",
    "        patterns = [\n",
    "            r'[A-Z]{2,}\\s*[A-Z0-9]{6,}',  # e.g., \"MG08ACA16TA\", \"WUH722222ALE6L4\"\n",
    "            r'ST\\d{4,}[A-Z]{2}\\d{3,}[A-Z]?',  # Seagate pattern\n",
    "            r'WD[A-Z0-9]{6,}',  # WD pattern\n",
    "            r'MG\\d{2}[A-Z]{3}\\d{2}[A-Z]{2,}',  # Toshiba pattern\n",
    "        ]\n",
    "        \n",
    "        models = []\n",
    "        for pattern in patterns:\n",
    "            matches = re.findall(pattern, text_upper)\n",
    "            models.extend(matches)\n",
    "        \n",
    "        return models if models else None\n",
    "\n",
    "    # Add manufacturer and model hints to reviews dataframe\n",
    "    df_reviews['manufacturer'] = df_reviews['title'].apply(extract_manufacturer)\n",
    "    df_reviews['manufacturer'] = df_reviews['manufacturer'].fillna(\n",
    "        df_reviews['text'].apply(extract_manufacturer)\n",
    "    )\n",
    "\n",
    "    df_reviews['model_hints'] = df_reviews['title'].apply(extract_model_hints)\n",
    "    df_reviews['model_hints'] = df_reviews['model_hints'].fillna(\n",
    "        df_reviews['text'].apply(extract_model_hints)\n",
    "    )\n",
    "\n",
    "    # Add manufacturer to backblaze data for easier joining\n",
    "    def get_bb_manufacturer(model):\n",
    "        \"\"\"Extract manufacturer from Backblaze model name\"\"\"\n",
    "        if model.startswith('TOSHIBA'):\n",
    "            return 'TOSHIBA'\n",
    "        elif model.startswith('ST') or model.startswith('SEAGATE'):\n",
    "            return 'SEAGATE'\n",
    "        elif model.startswith('WDC') or model.startswith('WD'):\n",
    "            return 'WDC'\n",
    "        elif model.startswith('HGST') or model.startswith('HITACHI'):\n",
    "            return 'HGST'\n",
    "        return None\n",
    "\n",
    "    df_backblaze['manufacturer'] = df_backblaze['model'].apply(get_bb_manufacturer)\n",
    "\n",
    "    print(\"Reviews with manufacturer extracted:\")\n",
    "    print(df_reviews[['parent_asin', 'title', 'manufacturer', 'model_hints']].head(10))\n",
    "    print(f\"\\nReviews with manufacturer identified: {df_reviews['manufacturer'].notna().sum():,} / {len(df_reviews):,}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "    print(\"Backblaze with manufacturer extracted:\")\n",
    "    print(df_backblaze[['model', 'manufacturer']].head(10))\n",
    "    print(f\"\\nManufacturer distribution in Backblaze:\")\n",
    "    print(df_backblaze['manufacturer'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379c4625",
   "metadata": {},
   "source": [
    "## Join Strategy Summary\n",
    "\n",
    "Now both datasets have a `manufacturer` field that can be used for joining:\n",
    "\n",
    "- **Reviews**: `df_reviews['manufacturer']` - extracted from title/text (TOSHIBA, SEAGATE, WDC, HGST)\n",
    "- **Backblaze**: `df_backblaze['manufacturer']` - extracted from model name\n",
    "\n",
    "You can join on manufacturer to analyze reviews by manufacturer against Backblaze failure data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bca1817a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if reload_s3:\n",
    "    # Example join: Get reviews by manufacturer\n",
    "    print(\"Join Example - Reviews by Manufacturer:\")\n",
    "    print(\"\\nReviews distribution:\")\n",
    "    print(df_reviews['manufacturer'].value_counts())\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "    print(\"Backblaze distribution:\")\n",
    "    print(df_backblaze['manufacturer'].value_counts())\n",
    "\n",
    "    # You can now join like this:\n",
    "    # joined_df = df_reviews.merge(df_backblaze, on='manufacturer', how='inner')\n",
    "    # Or do aggregations by manufacturer before joining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "89637555",
   "metadata": {},
   "outputs": [],
   "source": [
    "if reload_s3:\n",
    "    # Check a sample Backblaze parquet file to understand the schema\n",
    "    import pandas as pd\n",
    "    import pyarrow.parquet as pq\n",
    "    from io import BytesIO\n",
    "\n",
    "    # List some parquet files\n",
    "    s3_path = \"curated/backblaze_parquet/\"\n",
    "    response = s3.list_objects_v2(Bucket=bucket, Prefix=s3_path, MaxKeys=100)\n",
    "\n",
    "    if 'Contents' in response:\n",
    "        # Find the first actual parquet file\n",
    "        parquet_files = [obj['Key'] for obj in response['Contents'] if obj['Key'].endswith('.parquet')]\n",
    "        \n",
    "        if parquet_files:\n",
    "            sample_file = parquet_files[0]\n",
    "            print(f\"Reading schema from: {sample_file}\\n\")\n",
    "            \n",
    "            # Download and read parquet file metadata\n",
    "            obj = s3.get_object(Bucket=bucket, Key=sample_file)\n",
    "            buffer = BytesIO(obj['Body'].read())\n",
    "            \n",
    "            # Read parquet schema\n",
    "            parquet_file = pq.ParquetFile(buffer)\n",
    "            arrow_schema = parquet_file.schema_arrow\n",
    "            \n",
    "            print(f\"Number of columns: {len(arrow_schema.names)}\\n\")\n",
    "            print(\"First 20 columns:\")\n",
    "            for i, name in enumerate(arrow_schema.names[:20], 1):\n",
    "                field = arrow_schema.field(name)\n",
    "                print(f\"  {i}. {name} ({field.type})\")\n",
    "            \n",
    "            print(f\"\\n... and {len(arrow_schema.names) - 20} more columns\")\n",
    "        else:\n",
    "            print(f\"No parquet files found in s3://{bucket}/{s3_path}\")\n",
    "    else:\n",
    "        print(f\"No objects found in s3://{bucket}/{s3_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
